\section{Taylor Polynomials}We want an easier way of calculating a difficult function.  To this end we want to find a function that is similar to our original that we can calculate.  Taylor polynomials are one such type of functions with an easy calculation and intuition.  To find the Taylor polynomials we match the derivatives of the two polynomials at a particular point.  We are in essence enforcing a smoothness criterion at the point of interest.Thus the general expression for the Taylor series is:ExampleProblem 1.1-3(c) ExampleProblem 1.1-8 RemainderThe Taylor Series obviously has errors in its approximation.  If the original function is in $C_{n+1}$ on the interval _²x²_ (with $a$ in the interval) then the remainder (or error) is given bywith cx between a and x.  To get an error bound we assume that cx is the worst possible.ExampleProblem 1.2-3(a)In this case n=1 so the worst case would be if cos(c) were -1.ExampleProve problem 8.Multiplying PolynomialsStraightforward:a1*x*x*...*xThis takes k multiplications for a monomial of size k. so for a polynomial with monomials up to size n it would take n(n+1)/2 multiplications.Storing:Calculate x2=x*x, x3=x*x2, etc.This takes 2n-1 multiplications.Nesting:b0=a0+xb1b1=a1+xb2bn=anEach step takes 1 multiply so this method takes only n multiplications.The real savings come when you have to calculate a large polynomial many times.BinaryIn any number system, the position of a digit relative to the decimal place specifies the integer power of the base we must multiple the digit by to get its value.  So for base 10,and for base 2.This gives us one way to convert numbers.  For instance, we can convert binary to decimal by expanding the binary number in this way.  Thus using the above to convert binary (10.01) to decimal we find,Note that the "2" we are using is the base of binary in decimal form, and this is why we went from binary to decimal.  In binary, its form would be "10" and ten would be "1010". Therefore, we could go to binary by, expanding this out with ten in binary.  The problem with this method is it is clumsy to use since we do not do squaring, cubing, etc. easily in base 2.  Another problem is that 0.1 is an infinitely repeating decimal in binary so it is a pain to deal with 10-1!  Instead, we convert decimal to binary as follows.  1) Split your number into a.b2) For the whole number part (a)a) Divide 2 into a and note the quotient and remainder as q1,r1 (a=2*q1+r1)b) As long as the quotient from above is not zero, divide it by 2 and record the quotient and remainder as qi,ri (with i denoting the current step).  Repeat.c) The binary equivalent of a is rnrn-1...r2r1.  Basically we have done our nested polynomial evaluation backwards with x=2, and the coefficients being the remainders.3) For the fractional part (b)a) Multiply 2*b, and record the unit value as a1.  Denote b-a1=b1.b) If bi does not equal zero, multiply it by 2, denoting the units digit by ai+1 and the difference bi-ai+1=bi+1.  Repeat until the difference is zero (this may never happen so be looking for patterns to get repeating fractions).c) The fractional part, b, is a1a2a3a4...4) The full answer is thus rnrn-1...r2r1.a1a2a3a4...HexadecimalThis is often made to sound more intimidating than it is.  Hexadecimal numbers are simply base 16, but this can be handled nicely since 24=16.  All you have to do is group binary digits into groups of 4 and use the conversion table:BinHexDecBinHexDec0000001000880001111001990010221010A100011331011B110100441100C120101551101D130110661110E140111771111F15Floating point numbersWhile the book discusses single precision numbers, they are essentially never used, as double precision is so much better and readily available.  We will assume IEEE double precision floating point representation, as it is the standard.  IEEE floating point numbers have the form,whereSingle PrecisionDouble PrecisionP2453Emin-126-1022Emax1271023Bias1271023Thus IEEE is represented in memory as a sign bit, exponent bits (8 or 11), and mantissa bits (23 or 52).  The mantissa is composed of all the bj.  A few things to note about IEEE arithmetic.1. The exponent stored is E=e-Bias2. ±0 is encoded by Emin-1 and f=03. Denormalized numbers are encoded by Emin-1 and f­04. ±_ is encoded by Emax+1 and f=05. NAN is encoded by Emax+1 and f­0Approximating the RealsTo approximate the real number x, we define the function fl(x) as, 0 when x=0, and the nearest element in floating point to x otherwise.  Finding nearest elements requires a rounding scheme (rounding or "chopping"/truncating) and a tie breaker procedure (usually round away from zero).Bounding ErrorsTo bound the error in approximating the real number x, we need to consider the floating point number, fl(x), used to approximate x.  First we note that a real number x, is written in binary as,where s is the sign, f has as many digits as needed, and e is any integer.  Note that e will be different for IEEE, which normalizes to 1²fr<2 with an implicit 1 at the start; than the non-standard forms, which normalize to 0.5² fr<1 with no assumed leading 1.  We will assume that e is within the permitted bounds for simplicity.  The floating-point representation is .We can now write the difference asFor the moment, we will consider the difference (fr-f).  Note that we are dealing with normalized numbers with n bits of accuracy and an implicit leading 1 (IEEE arithmetic), while the book deals with numbers normalized between a half and one, with no implicit 1, so for us.Note that the digit to the left of the decimal in f is assumed to be 1, the only exception is when fr=1.1111... which would have f=10.000...0.  Technically it would actually have f=1.000...0 and the exponent would be (e+1) but since we are keeping the exponent e we keep the simplification.  Note that this is equivalent to rounding 9.5 to 10.  Anyway, our real concern is the worst case of the difference, which is in all cases given by.Note that the 1 is in the (n+1)st place after the decimal.  We rewrite this using floating point notation as.We now stick this back into the expression for the difference between x and fl(x) and obtain an upper bound by taking absolute value.Similarly to get a lower bound we take the negative of the absolute value, and findNow we note that the size of x is.For the book's form of the mantissa, we would have.The relative error is thus.Matlab ProgrammingThere are two basic ways to interact with Matlab: command line execution, and M-files. Yes there are others such as MEX-files, Simulink, and several interfacing programs, but they are not relevant to us.  We will primarily be concerned with the use of M-files, because they are the most helpful.  Command line execution is really just for quick operations and checking of segments of code.  Matlab syntax is a high level programming language that interacts with a series of numerical libraries (most notably LinPack, EisPack, and BLAS).  Like most programming languages we have two types of programs that can be written.  A regular program, which is written as you would type commands on the command line, is the most basic type and is often the way you will start homework problems and other projects.  Functions, which are sub-programs called by another program (even recursively by other functions), are probably the most useful, as they allow you to extend the language by defining new operations.  One of the main goals of this class is for you to walk away with a library of Matlab functions that you can use to do a variety of tasks.  So how do you specify which you want?  You will get a regular program unless you start the M-file with the command function.  The syntax isfunction a=name(x,y,..., z)orfunction [a,b,...,c]=name(x,y,..., z)The second form returns multiple values.Matlab gives us several command structures also: for, while, and if-elseif-else.  To see how these work lets use the programs I passed out last time as an example.Homework:Convert the Fortran program in 3.1 into Matlab syntax.Do problems 9, 13, 14 from section 3.1Propagation of ErrorWe have seen that representing the real numbers on a computer involves errors.  When we use floating point numbers in a calculation rather than the actual numbers the errors can grow.  The errors caused by using floating point approximations are called propagated errors.  Two ways of bounding propagation errors exist.  The forward method involves explicitly calculating the errors and is called interval arithmetic.  The backward method involves finding a condition number, which gives a bound on how big the error can grow.Interval ArithmeticLet's consider the error in a computation between the true values (xT, yT) and the approximate values (xA, yA).  We only know the approximate values and the error boundsNote that the error is could be positive or negative so we must consider the positive and negative bounds.  First, we will look at the error for addition or subtraction.Now let's consider multiplication.  It is easy to see that this can quickly become very hard to deal with.  Consider for instance multiplying two n-by-n matrices, which would involve $n^{3}$ multiplies.  Keeping track of all of them would rapidly become impossible.  We will consider one final operation, namely division.Again we can see that things can become very complicated quickly.Condition NumberWe will now consider the problem of evaluating a function, f(x), at an approximate rather than true value.  To do this we will require our function to be continuous on [xT,xA] and differentiable on (xT,xA).  We can thus use the mean value theorem to seeWe now note that since c is between the true and approximate values, and that the interval is on the order of 10-16 for IEEE double-precision arithmetic.  We can thus assume c is approximately xA.The derivative of f(x) at xA, is called the condition number and shows how the error of the approximation will influence the error of the calculation.  The condition number is nice in that it cleanly handles the error bounds.  It is not as precise as the error in the interval arithmetic, but it is tractable even for large matrix operations, which will involve the norms of the matrices rather than the elements.  Quite a savings!SumsWe have spoken a lot about summation, but we want to look at one final area of sums before we move on.  Consider the following summation:In real numbers it doesn't matter if we add the 45's first or the 100000.  In floating point numbers it does matter!  Floating point numbers are not associative.  To see this consider a 4 decimal place accuracy machine that uses rounding, and is nicely implemented.  In this case we see that 100000+45=100000so if we add as stated we find the sum is 100000 for the series (rather than 100180).  If we add the 45's first we find that 45+45+45+45=180.Then 100000+180=100200.A much better result.  These sums occur in a variety of places, from standard series, to evaluating integrals, to inner products of vector, and matrix multiplication.  In short you should be aware of the lack of the associative property.