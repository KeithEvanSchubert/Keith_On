We will now look at the problem of finding a polynomial to fit a set of points.  The points could come from measurements in an experiment, or it could come from a complex function we want to approximate.  In either case we will begin by considering the case where we want our polynomial to be exact at these values.  An obvious question is why the emphasis on polynomials, when so many other functions exist.  Indeed we do see the use of other basis (sin and cos in Fourier for example), but still polynomials hold a special place in many applications.  One major reason is the Theorem of Wiestrass from Real Analysis.  It basically says that polynomials can approximate any function (assuming you use the entire basis).\section{Lagrange Interpolation Basis}Probably the nicest way to visualize the interpolation polynomials is to consider the Lagrange interpolation basis functions.  For the set of points, $\{x_{0}, x_{1}, \ldots, x_{n}\}$ define the following polynomial:\beqnL_{i}(x)=\frac{\prod_{j\ne i}(x-x_{j})}{\prod_{j\ne i}(x_{i}-x_{j})}.\eeqnWe note in particular that $L_{i}(x_{j})=\delta_{i,j}$, which allows us to get the interpolation polynomial nicely.  The interpolating polynomial is then given by\beqnP_{n}(x)=\sum_{i=0}^{n}y_{i}L_{i}(x).\eeqnThe importance of the Lagrange basis giving us the Kronecker delta function cannot be over-emphasized, as it is the essential idea in getting the solution.Often the points are selected to be evenly spaced due to constraints in the basic system.  While this is not the best for errors, it is often a physical necessity (for example many data samplers are constrained this way).  In this case we can simplify the expression using \beqn\mu=\frac{x-x_{0}}{x_{1}-x_{0}}.\eeqnThis is covered well in the book.\section{Divided Difference}Divided difference is a similar method to Taylor approximation but instead of matching derivatives exactly at a point, it nearly approximates the derivative to exactly match certain points.  The result is the same as Lagrange's formula.\beqnF[x_{0},x_{1}] & = & \frac{f(x_{1})-f(x_{0})}{x_{1}-x_{0}} \\F[x_{0},x_{1},\cdots,x_{n}] & = & \frac{F[x_{1},\cdots,x_{n}]-F[x_{0},\cdots,x_{n-1}]}{x_{n}-x_{0}}\eeqn\beqnP_{1}(x) & = & f(x_{0})+(x-x_{0})F[x_{0},x_{1}] \\P_{k+1} & = & P_{k}+(x-x_{0})(x-x_{1})\cdots(x-x_{k})F[x_{0},x_{1},\cdots,x_{k+1}]\eeqnHomework:Section 5.1: 5, 9, 13Section 5.2: 2, 3, 7\section{Error}The key area to note from here is that the error is given by either of the following formulas.\beqnf(x)-P_{n}(x) & = & \prod_{i=0}^{n}(x-x_{i})\frac{f^{(n+1)}(c_{x})}{(n+1)!} \\ & = & \prod_{i=0}^{n}(x-x_{i})F[x_{0},x_{1},\cdots,x_{n},x]\eeqnThe important part of this is to note that these are themselves polynomials of order $n+1$.  Consider the plot of a polynomial with equi-spaced roots.  It is trivial to note that the height of the peaks between the roots is bigger towards the outside of the interval.\section{Splines}For splines we want to fit a cubic polynomial for each interval so that the first and second derivatives between two sections match on the boundary.  Following the books derivation we get the formula for the polynomial on the interval $[x_{j-1},x_{j}]$ to be\beqns(x) & = &        a_{1}(x_{j}-x)^{3}+a_{0}(x-x_{j-1})^{3}      +b_{1}(x_{j}-x)+b_{0}(x-x_{j-1}) \\a_{i} & = & \frac{M_{j-i}}{6(x_{j}-x_{j-1})} \\b_{i} & = & \frac{y_{j-i}-\frac{1}{6}M_{j-i}(x_{j}-x_{j-1})^{2}}{(x_{j}-x_{j-1})}\eeqnThe only thing that we need is to calculate $M_{i}$ for the natural cubic spline, which is done by requiring $M_{1}=M_{n}=0$ and solving the following matrix system\beqnAx & = & b \\A & = & \left[\matrix{\alpha_{2} & \beta_{2}  &  0        & \cdots       & 0 \cr\beta_{2}  & \alpha_{3} & \beta_{3} & \ddots       & \vdots \cr0          & \beta_{3}  & \ddots    & \ddots       & 0 \cr\vdots     & \ddots     & \ddots    & \alpha_{n-2} & \beta_{n-2} \cr0          & \cdots     & 0         & \beta_{n-2}  & \alpha_{n-1}}\right] \\x & = & \left[\matrix{M_{2} \cr\vdots \crM_{n-1}}\right] \qquad b = \left[\matrix{\gamma_{2}-\gamma_{1} \cr\vdots \cr\gamma_{n-1}-\gamma_{n-2}}\right] \\\alpha_{i} & = & \frac{x_{i+1}-x_{i-1}}{3}\qquad\beta_{i} = \frac{x_{i+1}-x_{i}}{6}\qquad\gamma_{i} = \frac{y_{i+1}-y_{i}}{x_{i+1}-x_{i}}\eeqnWe can also find the $M_{i}$ for the not-a-knot cubic spline, which is often preferred by solving a similar system\beqnAx & = & b \\A & = & \left[\matrix{\psi_{1}   & \beta_{1}  &  0         &  0         & \cdots       & 0            & 0\cr\beta_{1}  & \alpha_{2} & \beta_{2}  &  0         & \cdots       & 0            & 0 \cr0          & \beta_{2}  & \alpha_{3} & \beta_{3}  & \ddots       & \vdots       & \vdots \cr0          & 0          & \beta_{3}  & \ddots     & \ddots       & 0            & 0 \cr\vdots     & \vdots     & \ddots     & \ddots     & \alpha_{n-2} & \beta_{n-2}  & 0 \cr0          & 0          & \cdots     & 0          & \beta_{n-2}  & \alpha_{n-1} & \beta_{n-1} \cr0          & 0          & \cdots     & 0          & 0            & \beta_{n-1}  & \phi_{2}}\right] \\x & = & \left[\matrix{M_{1} \crM_{2} \cr\vdots \crM_{n-1} \crM_{n}}\right] \qquad b = \left[\matrix{\gamma_{1}-f'(x_{1}) \cr\gamma_{2}-\gamma_{1} \cr\vdots \cr\gamma_{n-1}-\gamma_{n-2} \crf'(x_{n})-\gamma_{n-1}}\right] \\\alpha_{i} & = & \frac{x_{i+1}-x_{i-1}}{3}\qquad\beta_{i} = \frac{x_{i+1}-x_{i}}{6}\qquad\gamma_{i} = \frac{y_{i+1}-y_{i}}{x_{i+1}-x_{i}} \\\psi_{1} & = & \frac{x_{2}-x_{1}}{3}\qquad\phi_{2} = \frac{x_{n}-x_{n-1}}{3}  \eeqnor (if you don't know the derivative) \beqnAx & = & b \\A & = & \left[\matrix{\psi_{1}   & \psi_{2}   &  0         &  0         & \cdots       & 0            & 0 \cr\beta_{1}  & \alpha_{2} & \beta_{2}  &  0         & \cdots       & 0            & 0 \cr0          & \beta_{2}  & \alpha_{3} & \beta_{3}  & \ddots       & \vdots       & \vdots \cr0          & 0          & \beta_{3}  & \ddots     & \ddots       & 0            & 0 \cr\vdots     & \vdots     & \ddots     & \ddots     & \alpha_{n-2} & \beta_{n-2}  & 0 \cr0          & 0          & \cdots     & 0          & \beta_{n-2}  & \alpha_{n-1} & \beta_{n-1} \cr0          & 0          & \cdots     & 0          & 0            & \phi_{2}     & \phi_{1}}\right] \\x & = & \left[\matrix{M_{1} \crM_{2} \cr\vdots \crM_{n-1} \crM_{n}}\right] \qquad b = \left[\matrix{\psi_{3} \cr\gamma_{2}-\gamma_{1} \cr\vdots \cr\gamma_{n-1}-\gamma_{n-2} \cr\phi_{3}}\right] \\\alpha_{i} & = & \frac{x_{i+1}-x_{i-1}}{3}\qquad\beta_{i} = \frac{x_{i+1}-x_{i}}{6}\qquad\gamma_{i} = \frac{y_{i+1}-y_{i}}{x_{i+1}-x_{i}} \\\xi_{1} & = & x_{2}-x_{1}\qquad\xi_{2} = x_{2}-z_{1}\qquad\xi_{3} = z_{1}-x_{1} \\\psi_{1} & = & \frac{\xi_{2}^{3}-\xi_{1}^{2}\xi_{2}}{6\xi_{1}}\qquad\psi_{2} = \frac{\xi_{3}^{3}-\xi_{1}^{2}\xi_{3}}{6\xi_{1}} \qquad\psi_{3} = f(z_{1})-\frac{\xi_{2}y_{1}+\xi_{3}y_{2}}{\xi_{1}} \\\xi_{4} & = & x_{n}-x_{n-1}\qquad\xi_{5} = x_{n}-z_{2}\qquad\xi_{6} = z_{2}-x_{n-1} \\\phi_{1} & = & \frac{\xi_{5}^{3}-\xi_{4}^{2}\xi_{5}}{6\xi_{4}}\qquad\phi_{2} = \frac{\xi_{6}^{3}-\xi_{4}^{2}\xi_{6}}{6\xi_{4}}\qquad\phi_{3} = f(z_{2})-\frac{\xi_{5}y_{n-1}+\xi_{6}y_{n}}{\xi_{4}}\eeqnNote, you can easily enter the matrix $A$ into Matlab by using the command diag.  For instance, if you put the entries of $A$ that are on the main diagonal into the vector $A1$, the first sub-diagonal into $A2$, and the first super-diagonal into $A3$, then in Matlab you enter,{\it A=diag(A1)+diag(A2,-1)+diag(A3,1);}.Homeworksection 5.3: 7section 5.4: 3, 5\newpage\section{Least Squares Approximation}Up till know we have dealt with interpolation, where we want to exactly match a set of points.  In reality, we are often more concerned with having a good overall approximation rather than an exact matching at a few points.  There are a lot of ways to approximate a function.  In general there are two main areas discrete and continuous.  We will cover the discrete case.  The continuous method involves some functional analysis and we do not have the time to cover it well.  If you are interested it can provide a fun project, and I have some good resources you can use.We proceed with the discrete case.  The discrete case involves measuring the function to be approximated at a series of points, and then finding the best coefficients in some sense for some functions of interest.  Some sense?  What do I mean by that?  Well, put simply, there are a variety of different methods of measuring how good an approximation is.  The standard method is the one we will concentrate on, and it is called least squares.  As with many things in Math, least squares owes its basis to Gauss.  The basic idea is to reduce the sum of the squares of the distances from the measurements to the function to be fitted at each of the x values.  The last point is very important because it is the basis of much of the problems in least squares.  In essence the answer you get is dependent on your choice of independent variables.  Below is an excerpt from my dissertation which covers what we are talking about now.  The key idea to get is that there are reasons to look beyond least squares.Consider the problem of calibrating a gas thermometer.  Gas thermometers are based on Charles' law, which states that the volume of a fixed mass of gas at a fixed pressure is proportional to its temperature.  A simple gas thermometer can be made by trapping some gas with a mercury plug in a capillary tube that is open on only one end \bb{GenChem}.  The volume is thus proportional to the height of the plug.  The equation of the thermometer is thus $hc_{1}=T$, where $h$ is the height of the plug, $c_{1}$ is the constant we want to know, and $T$ is the absolute temperature.  We place the gas thermometer in a stirred liquid bath with a known thermometer.  We heat the bath and take height and temperature measurements at various times.  The LS solution gives us that $\hat c_{1}=h^{\dagger}T$, but we can see that this minimizes the error in the measured temperature, $T$, from the predicted temperature, $hh^{\dagger}T$.  By the same token we could use the relation $h=c_{2}T$, with $c_{2}=\frac{1}{c_{1}}$.  The LS solution, $\hat c_{2}=T^{\dagger}h$, thus minimizes the error between the measured height, $h$, and the predicted height $TT^{\dagger}h$.  A problem arises in the LS method in that generally $\hat c_{1}\ne\frac{1}{\hat c_{2}}$.  This can be seen easily in Figure~\ref{gastherm}.  The slope of the line designated temperature errors, is $\hat c_{1}$, while the slope of the line designated height errors is $\frac{1}{\hat c_{2}}$.  The line designated theoretical is the ``true'' system from which the estimates were generated.  It is easy to see that the slopes are not the same, and thus $\hat c_{1}\ne\frac{1}{\hat c_{2}}$.  The LS solution does not even perfectly handle the case where the system matrix is ``known'', which gives us cause to be concerned as to how it will perform when there are perturbations to the system matrix.\begin{figure}[h]\begin{center}\leavevmode\hbox{\epsfxsize=4in\epsffile{gastherm.eps}}\end{center}\caption{Gas Thermometer Example}\label{gastherm}\end{figure}The most well known alternative to least squares is total least squares (TLS).  In TLS we look at the perpendicular distance to the function.  This handles many of the problems of least squares but is more sensitive to errors, as it is ``optimistic'' in how it looks at the problem.  A huge body of literature is dedicated to this problem, and this is the central area of my dissertation.  While some of these other methods are very interesting, we will stick to least squares for the moment, but we will remember that problems can occur and so if we have problems we know there are things we can do.Getting back to business we have a set of $m$ points $(x_{i},y_{i})$ and a group of $n$ functions $\phi_{i}(x)$ that we want to use to approximate the points with.  We thus have $m$ equations to find $n$ coefficients.\beqny_{1} & - & \sum_{i=1}^{n}a_{i}\phi_{i}(x_{1}) \\y_{2} & - & \sum_{i=1}^{n}a_{i}\phi_{i}(x_{2}) \\& \vdots & \\y_{m} & - & \sum_{i=1}^{n}a_{i}\phi_{i}(x_{m})\eeqnWe can rewrite these into a matrix formulation, as\beqnY-\Phi A \\\eeqnwhere \beqnY & = & \left[\matrix{y_{1} & y_{2} & \cdots & y_{m}}\right]^{T} \\\Phi & = & \left[\matrix{\phi_{1}(x_{1}) & \phi_{2}(x_{1}) & \cdots & \phi_{n}(x_{1}) \cr\phi_{1}(x_{2}) & \phi_{2}(x_{2}) & \cdots & \phi_{n}(x_{2}) \cr\vdots          & \vdots          & \ddots & \vdots \cr\phi_{1}(x_{m}) & \phi_{2}(x_{m}) & \cdots & \phi_{n}(x_{m})}\right] \\A & = & \left[\matrix{a_{1} & a_{2} & \cdots & a_{m}}\right]^{T}.\eeqnAt this point we want to minimize the square error which is what the 2-norm does, so we have $\min_{A}\| Y-\Phi A\|_{2}^{2}$ .  The norm we are minimizing is called the cost function.  The solution is given by $A=\Phi^{\dagger}Y$, where $\Phi^{\dagger}$ is called the pseudo-inverse of $\Phi$.  Prove it?  Sure!  To avoid getting into some deeper areas of linear algebra we will assume that $\Phi$ has linearly independent columns.  This is not restrictive, as we usually have a lot of measurements and only a few functions we want to fit to them ($m>>n$).We recall from calculus that the minimum occurs when the gradient (derivative) is zero. We thus take the gradient of the cost with respect to $A$ and set it equal to zero to obtain \beqn0 & = & \nabla_{A}\| Y-\Phi A\|_{2}^{2} \\ & = & \nabla_{A}(Y-\Phi A)^{T}(Y-\Phi A) \\ & = & -\Phi^{T}(Y-\Phi A) \\ & = & \Phi^{T}\Phi A-\Phi^{T}Y \\\Phi^{T}Y & = & \Phi^{T}\Phi A\eeqnThe last line is what is referred to as the normal equation(s).  Note that some pluralize it to reflect that the single matrix equation reflects $n$ scalar equations.  I don't care, use what you like.  We note that if $\Phi$ has linearly independent columns, then $(\Phi^{T}\Phi)^{-1}$ exists.\beqn\Phi^{T}\Phi A & = & \Phi^{T}Y \\A & = & (\Phi^{T}\Phi)^{-1}\Phi^{T}Y \\A & = & \Phi^{\dagger}Y\eeqnYou might wonder how the last step works.  Some might just call it a definition but in reality it is because $(\Phi^{T}\Phi)^{-1}\Phi^{T}$ satisfies the four conditions of a pseudo inverse (called the Penrose conditions).  \begin{enumerate}\item $\Phi\Phi^{\dagger}\Phi=\Phi$\item $\Phi^{\dagger}\Phi\Phi^{\dagger}=\Phi^{\dagger}$\item $\Phi\Phi^{\dagger}=(\Phi\Phi^{\dagger})^{T}$\item $\Phi^{\dagger}\Phi=(\Phi^{\dagger}\Phi)^{T}$\end{enumerate}The properties are simple and easy to check, and yes, you have to check all four.  Many times a candidate matrix fails only one of them.  The first two properties tell us that it correctly maps the range spaces from the fundamental theorem of linear algebra, and the second two tell us the composite maps are symmetric.  The pseudo-inverse always exists and is unique.  Additionally, when the true inverse exists, it is the pseudo-inverse.  These are just a few of the many reasons to love the pseudo-inverse\ldotsThe result is established.  The nice thing about how we have handled things here is we have not specified what the functions are (they have to be linearly independent but that is no problem) or how many of them we want to fit.  You can now fit any combination of functions you like.As an example let's look at linear least squares for the points (0,1), (1,2), and (2,3).  We need to find the coefficients $m$, and $b$ for the line.  We construct our matrices\beqnY & = & \left[\matrix{1 & 2 & 3}\right]^{T} \\\Phi & = & \left[\matrix{1 & 1 & 1 \cr0 & 1 & 2 }\right]^{T} \\A & = & \left[\matrix{b & m}\right]^{T}.\eeqnAs a second example, consider fitting $e^{ax}$ to (0,1), (1,.5), and (2,.25).  To separate the coefficient, $a$, from the variable, $x$ we take the natural log of $y_{i}=e^{ax_{i}}$ to obtain $\ln(y_{i})=ax$.  We can proceed as before now.As a third example we will consider the second problem where we have noise (random errors) in the measurements.  These three examples are coded into Matlab by\begin{list}{}{\leftmargin=3em}\item[]\begin{verbatim}Y=[1;2;3];Ye=log([1;.5;.25]);Yee=log([1;.5;.25]+.3*rand(3,1));X=[0;1;2];One=ones(3,1);Phi=[One,X];A=Phi\Ynorm(Y-Phi*A)Ae=X\YeAee=X\YeeXf=0:.05:2;Yfe=exp(Ae.*Xf);Yfee=exp(Aee.*Xf);p1=[-.1,2.1];p2=[-.1,3.1];q=[0,0];subplot(3,1,1)plot(X,Y,'w*',X,Phi*A,'w-',p1,q,'w-',q,p2,'w-')axis([p1,p2])subplot(3,1,2)plot(X,exp(Ye),'w*',Xf,Yfe,'w-',p1,q,'w-',q,p2,'w-')axis([p1,p2])subplot(3,1,3)plot(X,exp(Yee),'w*',Xf,Yfee,'w-',p1,q,'w-',q,p2,'w-')axis([p1,p2])\end{verbatim}\end{list}and we get the output below and in Fig~\ref{llsqex}.\begin{list}{}{\leftmargin=3em}\item[]\begin{verbatim}A =    1.0000    1.0000ans =    0Ae =   -0.6931Aee =   -0.4650\end{verbatim}\end{list}\begin{figure}[h]\begin{center}\leavevmode\hbox{\epsfxsize=4in\epsffile{LinLeastSq1.eps}}\end{center}\caption{Least Squares Example}\label{llsqex}\end{figure}Homework 8.6: 1,3