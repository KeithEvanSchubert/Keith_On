\chapter{Matrix Preliminaries}\label{c-matrix}

Matrices are an essential tool in control systems design.  First lets discuss some basic terms.
\begin{description}
    \item[$\RE$] The real numbers.
    \item[$\REn$] The vectors composed of n-tuples of real numbers.
    \item[$\REmn$] The matrices composed of m rows and n columns of real numbers.
\end{description}

\section{Addition and Subtraction}

In order to add or subtract matrices, the dimensions must be the same.  Essentially we can add two $\REmn$ matrices but not a $\REmn$ and a $\RE^{p\times r}$ matrix or even a $\REmn$ and a $\REmm$ matrix.  Addition (or subtraction) is done by adding (or subtracting) the corresponding elements in the two matrices.  For two $\RE^{3\times 2}$ matrices addition is given by
\begin{eqnarray}
\bmat
1 & 2 \\
3 & 4 \\
5 & 6
\emat
+
\bmat
7  & 8 \\
9  & 10 \\
11 & 12
\emat
=
\bmat
1+7  & 2+8 \\
3+9  & 4+10 \\
5+11 & 6+12
\emat
=
\bmat
8   & 10 \\
12  & 14 \\
16  & 18
\emat
\end{eqnarray}
For two $\RE^{3\times 2}$ matrices addition is given by
\begin{eqnarray}
\bmat
1 & 2 \\
3 & 4 \\
5 & 6
\emat
-
\bmat
6  & 5 \\
4  & 3 \\
2  & 1
\emat
=
\bmat
1-6  & 2-5 \\
3-4  & 4-3 \\
5-2  & 6-1
\emat
=
\bmat
-5  & -3 \\
-1  & 1  \\
3   & 5
\emat
\end{eqnarray}


\section{Multiplication}

In order to do multiplication, we need to have matrices that are compatible to multiply.  Recall that in order to add two matrices they had to be the same size.  Unfortunately this is not the condition for multiplication.  To be able to multiply a matrix $A$ on the right by a matrix $B$\footnote{Multiplying a matrix $A$ on the right by a matrix $B$ means $AB$, left multiplication by $B$ would be $BA$.  In matrices it is not true in general that $AB=BA$, or even that one can be calculated even if the other exists.  This will make more sense in a few seconds.}, we must have that the inner matrix dimensions are equal.  That is, we require that $A\in\RE^{m\times p}$ and $B\in\RE^{p\times n}$, Where $m$, $n$, and $p$ do not have to be the same, but they could.  Thus we could multiply the following
\begin{itemize}
    \item $AB$ or $BA$ with $\{A,B\}\in\RE^{3\times 3}$;
    \item $AB$ with $A\in\RE^{3\times 3}$ and $B\in\RE^{3\times 4}$;
    \item $AB$ with $A\in\RE^{2\times 3}$ and $B\in\RE^{3\times 4}$.
\end{itemize}
Two general ways to do multiplication exist.  Each has computational advantages in different situations.  They give the same answer they are just different ways to group the solution.

\subsection{Inner Product}

A full study of the inner product is beyond the scope of this work, but interested students are referred to texts on Hilbert Spaces\footnote{Hilbert Spaces are spaces with a defined inner product.  They play an important role in control systems theory}.  An inner product of two vectors of size n\footnote{A vector of size n is the same as saying the vector is in $\REn$.} $a$ and $b$ is given by $\left<a,b\right> = a^Tb=\sum_{i=1}^n(a_ib_i)$.  Thus it is the sum of the product of corresponding elements in the vectors.

\Example{
\begin{eqnarray*}
a=\bmat 1 & 2 & 3 \emat^T
\qquad
b=\bmat 4 & 5 & 6 \emat^T
\end{eqnarray*}
Note that I have defined the vectors in the transposed form to save space.  The transpose just means
\begin{equation}
\bmat 1&2\emat^T =\bmat 1 \\ 2\emat.
\end{equation}
Then the inner product of $a$ and $b$ is
\begin{eqnarray*}
    \left<a,b\right>
    &=& \bmat 1 & 2 & 3 \emat\bmat 4 \\ 5 \\ 6 \emat \\
    &=& 1\times 4 + 2\times 5 + 3\times 6 \\
    &=& 32
\end{eqnarray*}}

Now try it in SciLab.  First define $a$ and $b$ by
\begin{verbatim}
--> a=[1;2;3];
--> b=[4;5;6];
\end{verbatim}
Note that I ended each line with a ``;'', which tells SciLab to suppress its responses.  This is vital when using the programming mode or SciLab will scroll out lots of unneeded info.  With the vectors described we just need to take the inner product.  To do this we use the $a^Tb$ form, which is written
\begin{verbatim}
--> a'*b
\end{verbatim}
The prime(') does the transpose ($^T$) and the multiply then does the inner product.

With vector inner products down we can consider two matrices, $A\in\RE^{4\times 2}$ and $B\in\RE^{2\times 3}$.
\begin{eqnarray*}
A =\bmat
a_{1,1} & a_{1,2} \\
a_{2,1} & a_{2,2} \\
a_{3,1} & a_{3,2} \\
a_{4,1} & a_{4,2}
\emat
\qquad B=\bmat
b_{1,1} & b_{1,2} & b_{1,3} \\
b_{2,1} & b_{2,2} & b_{2,3}
\emat
\end{eqnarray*}
We will consider partitioning the matrices into vectors that can be multiplied.  We define
\begin{itemize}
    \item The first row of $A$ to be $a_1=\bmat a_{1,1} & a_{1,2}\emat$.
    \item The second row of $A$ to be $a_2=\bmat a_{2,1} & a_{2,2}\emat$.
    \item The third row of $A$ to be $a_3=\bmat a_{3,1} & a_{3,2}\emat$.
    \item The fourth row of $A$ to be $a_4=\bmat a_{4,1} & a_{4,2}\emat$.
    \item The first column of $B$ to be $b_1=\bmat b_{1,1} & b_{2,1}\emat^T$.
    \item The second column of $B$ to be $b_2=\bmat b_{1,2} & b_{2,2}\emat^T$.
    \item The third column of $B$ to be $b_3=\bmat b_{1,3} & b_{2,3}\emat^T$.
\end{itemize}
then
\begin{equation}
A=\bmat
a_1 \\
a_2 \\
a_3 \\
a_4
\emat \qquad
B=\bmat b_1 & b_2 & b_3\emat
\end{equation}
and so the product
\begin{eqnarray*}
AB &=&
\bmat
a_1 \\
a_2 \\
a_3 \\
a_4
\emat
\bmat b_1 & b_2 & b_3\emat \\
&=&
\bmat
a_1b_1 & a_1b_2 & a_1b_3 \\
a_2b_1 & a_2b_2 & a_2b_3 \\
a_3b_1 & a_3b_2 & a_3b_3 \\
a_4b_1 & a_4b_2 & a_4b_3
\emat \\
&=&
\bmat
\sum_{i=1}^{2}a_{1,i}b_{i,1} & \sum_{i=1}^{2}a_{1,i}b_{i,2} & \sum_{i=1}^{2}a_{1,i}b_{i,3} \\
\sum_{i=1}^{2}a_{2,i}b_{i,1} & \sum_{i=1}^{2}a_{2,i}b_{i,2} & \sum_{i=1}^{2}a_{2,i}b_{i,3} \\
\sum_{i=1}^{2}a_{3,i}b_{i,1} & \sum_{i=1}^{2}a_{3,i}b_{i,2} & \sum_{i=1}^{2}a_{3,i}b_{i,3} \\
\sum_{i=1}^{2}a_{4,i}b_{i,1} & \sum_{i=1}^{2}a_{4,i}b_{i,2} & \sum_{i=1}^{2}a_{4,i}b_{i,3}
\emat
\end{eqnarray*}
By hand the easiest way to do this is to line up the two matrices to multiply as follows
\begin{eqnarray*}
\bmat
b_{1,1} & b_{1,2} & b_{1,3} \\
b_{2,1} & b_{2,2} & b_{2,3}
\emat \\
\bmat
a_{1,1} & a_{1,2} \\
a_{2,1} & a_{2,2} \\
a_{3,1} & a_{3,2} \\
a_{4,1} & a_{4,2}
\emat
\bmat
\fbox{\hspace{8pt}} & \fbox{\hspace{8pt}} & \fbox{\hspace{8pt}} \\
\fbox{\hspace{8pt}} & \fbox{\hspace{8pt}} & \fbox{\hspace{8pt}} \\
\fbox{\hspace{8pt}} & \fbox{\hspace{8pt}} & \fbox{\hspace{8pt}} \\
\fbox{\hspace{8pt}} & \fbox{\hspace{8pt}} & \fbox{\hspace{8pt}}
\emat
\end{eqnarray*}
Then calculate each component by lining up the row and column and multiplying the corresponding terms in them.
\begin{eqnarray*}
\bmat
{\color{ans}b_{1,1}} & b_{1,2} & b_{1,3} \\
{\color{ans}b_{2,1}} & b_{2,2} & b_{2,3}
\emat \\
\bmat
{\color{ans}a_{1,1}} & {\color{ans}a_{1,2}} \\
a_{2,1} & a_{2,2} \\
a_{3,1} & a_{3,2} \\
a_{4,1} & a_{4,2}
\emat
\bmat
{\color{ans}a_{1,1}b_{1,1}+a_{1,2}b_{2,1}} & \fbox{\hspace{8pt}} & \fbox{\hspace{8pt}} \\
\fbox{\hspace{8pt}} & \fbox{\hspace{8pt}} & \fbox{\hspace{8pt}} \\
\fbox{\hspace{8pt}} & \fbox{\hspace{8pt}} & \fbox{\hspace{8pt}} \\
\fbox{\hspace{8pt}} & \fbox{\hspace{8pt}} & \fbox{\hspace{8pt}}
\emat
\end{eqnarray*}
The second element is
\begin{eqnarray*}
\bmat  % start second one
{\color{ans}b_{1,1}} & b_{1,2} & b_{1,3} \\
{\color{ans}b_{2,1}} & b_{2,2} & b_{2,3}
\emat \\
\bmat
a_{1,1} & a_{1,2} \\
{\color{ans}a_{2,1}} & {\color{ans}a_{2,2}} \\
a_{3,1} & a_{3,2} \\
a_{4,1} & a_{4,2}
\emat
\bmat
a_{1,1}b_{1,1}+a_{1,2}b_{2,1} & \fbox{\hspace{8pt}} & \fbox{\hspace{8pt}} \\
{\color{ans}a_{2,1}b_{1,1}+a_{2,2}b_{2,1}} & \fbox{\hspace{8pt}} & \fbox{\hspace{8pt}} \\
\fbox{\hspace{8pt}} & \fbox{\hspace{8pt}} & \fbox{\hspace{8pt}} \\
\fbox{\hspace{8pt}} & \fbox{\hspace{8pt}} & \fbox{\hspace{8pt}}
\emat
\end{eqnarray*}
The third element is
\begin{eqnarray*}
\bmat  % start second one
{\color{ans}b_{1,1}} & b_{1,2} & b_{1,3} \\
{\color{ans}b_{2,1}} & b_{2,2} & b_{2,3}
\emat \\
\bmat
a_{1,1} & a_{1,2} \\
a_{2,1} & a_{2,2} \\
{\color{ans}a_{3,1}} & {\color{ans}a_{3,2}} \\
a_{4,1} & a_{4,2}
\emat
\bmat
a_{1,1}b_{1,1}+a_{1,2}b_{2,1} & \fbox{\hspace{8pt}} & \fbox{\hspace{8pt}} \\
a_{2,1}b_{1,1}+a_{2,2}b_{2,1} & \fbox{\hspace{8pt}} & \fbox{\hspace{8pt}} \\
{\color{ans}a_{3,1}b_{1,1}+a_{3,2}b_{2,1}} & \fbox{\hspace{8pt}} & \fbox{\hspace{8pt}} \\
\fbox{\hspace{8pt}} & \fbox{\hspace{8pt}} & \fbox{\hspace{8pt}}
\emat
\end{eqnarray*}
The fourth element is
\begin{eqnarray*}
\bmat  % start second one
{\color{ans}b_{1,1}} & b_{1,2} & b_{1,3} \\
{\color{ans}b_{2,1}} & b_{2,2} & b_{2,3}
\emat \\
\bmat
a_{1,1} & a_{1,2} \\
a_{2,1} & a_{2,2} \\
a_{3,1} & a_{3,2} \\
{\color{ans}a_{4,1}} & {\color{ans}a_{4,2}}
\emat
\bmat
a_{1,1}b_{1,1}+a_{1,2}b_{2,1} & \fbox{\hspace{8pt}} & \fbox{\hspace{8pt}} \\
a_{2,1}b_{1,1}+a_{2,2}b_{2,1} & \fbox{\hspace{8pt}} & \fbox{\hspace{8pt}} \\
a_{3,1}b_{1,1}+a_{3,2}b_{2,1} & \fbox{\hspace{8pt}} & \fbox{\hspace{8pt}} \\
{\color{ans}a_{4,1}b_{1,1}+a_{4,2}b_{2,1}} & \fbox{\hspace{8pt}} & \fbox{\hspace{8pt}}
\emat
\end{eqnarray*}
Then repeat the process till every box is filled.


\section{Matrix Classification}

\subsection{Basic Properties}

\begin{tabular}{ll}
Property                    & Meaning \\\hline
Diagonal                    & $A_{ij}=0 \forall i\ne j$\\
Lower Triangular            & $A_{ij}=0 \forall i<j$\\
Upper Triangular            & $A_{ij}=0 \forall i>j$\\
Tridiagonal                 & $A_{ij}=0 \forall |i-j|\le 1$\\
Pentediagonal               & $A_{ij}=0 \forall |i-j|\le 3$\\
Lower Hessenberg            & $A_{ij}=0 \forall i+1>j$\\
Upper Hessenberg            & $A_{ij}=0 \forall i>j+1$\\
Symmetric                   & $A=A^T$\\
Normal                      & $AA^H = A^HA$\\
Idempotent                  & $A^2=A$\\
Orthogonal                  & $A^{-1}=A^{T}$\\
Unitary                     & $A^{-1}=A^{H}$\\
Positive Definite ($A>0$)   & $x^HAx>0 \forall x\in\mathbb{C}^n$ \\
\end{tabular}


\subsection{Definite}

\subsubsection{Positive Definite}

A matrix, $A\in\mathbb{C}^{n\times n}$ is defined as positive definite if $\forall x\in\mathbb{C}^n$, $x^HAx>0$.  Usually we further restrict this to symmetric matrices, as they are what comes up in practice and they are easier to handle (more algorithms, easier proofs, etc.), but they are not the only matrices which fit the definition\footnote{It is worth noting that there are non-symmetric positive definite matrices.  I have never seen anyone categorize how to create one, so I decided to do it in Theorem~\ref{theorem-non-sym-pd} on page~\pageref{theorem-non-sym-pd}.}.

Positive definite matrices are non-singular, and thus invertible.  In some sense they are the nicest matrices to work with.


\begin{theorem}\label{theorem-non-sym-pd}
Let $A$ be any symmetric positive definite matrix of size $n$, with smallest singular value $\sigma_n$.  Let $q_1$ and $q_2$ be two orthonormal n-vectors and $1>\alpha>0$ be a scalar. The matrix $A+n\alpha\sigma_n q_1q_2^H$ is non-symmetric positive definite.
\end{theorem}

\Pf
Let $Q$ be an orthogonal matrix with the first two columns $q_1$ and $q_2$ respectively.  $Q$ forms a basis for the n-vectors, so $\forall y\in\mathbb{C}^n \exists x\in\mathbb{C}^n$, $y=Qx$.  Since $Q$ is unitary it is invertible, so it is 1-1 and onto, thus every $x$ has a unique $y$ and vice versa.  Let the singular value decomposition of $A$ be $U\Sigma U^H$.  Now consider
\begin{eqnarray}
y^H(A+n\alpha\sigma_n q_1q_2^H)y
 &=& y^HAy+n\alpha\sigma_n y^Hq_1q_2^Hy\\
 &=& x^HQ^HAQx+n\alpha\sigma_n x^HQ^Hq_1q_2^HQx\\
 &=& x^HQ^HU\Sigma U^HQx+n\alpha\sigma_n x_1x_2\\
 &=& x^HV^H\Sigma Vx+n\alpha\sigma_n x_1x_2
\end{eqnarray}
for $V=U^HQ$.  Note that $V^H\Sigma V$ must be symmetric positive definite, thus $x^HV^H\Sigma Vx\geq\|x\|_2^2\sigma_n$.  If $x_1$ or $x_2$ are zero then it is trivial that $x^HV^H\Sigma Vx+n\alpha\sigma_n x_1x_2>0$, so it only remains to prove positive definiteness when both are not zero.  That means
\begin{eqnarray}
y^H(A+n\alpha\sigma_n q_1q_2^H)y
 &\geq& \|x\|_2^2\sigma_n+n\alpha\sigma_n x_1x_2\\
 &\geq& n\|x\|_\infty^2\sigma_n+n\alpha\sigma_n x_1x_2\\
 &\geq& n|x_1x_2|\sigma_n+n\alpha\sigma_n x_1x_2
\end{eqnarray}
Now factor this expression as follows
\begin{eqnarray}
n|x_1| |x_2|\sigma_n+n\alpha\sigma_n x_1x_2
 &=&n|x_1| |x_2|\sigma_n+n\alpha\sigma_n |x_1| |x_2|sign(x_1x_2)\\
 &=&(1+\alpha sign(x_1x_2))n|x_1| |x_2|\sigma_n.
\end{eqnarray}
Trivially, $n|x_1| |x_2|\sigma_n>0$, and $1+\alpha sign(x_1x_2)>0$, so $n|x_1| |x_2|\sigma_n+n\alpha\sigma_n x_1x_2>0$ and thus we have $y^H(A+n\alpha\sigma_n q_1q_2^H)y>0$. Since this holds for all $y$, we have that $A+n\alpha\sigma_n q_1q_2^H$ is positive definite.  The non-symmetry is a direct result of the structure.
\Pfend


%All non-symmetric positive definite matrices can be written in the form of $U(\Sigma +W)U^H$, where $A=U\Sigma U^H$ is the SVD of a symmetric positive definite matrix and W is a matrix such that $w_{ij}\geq 0$, $w_{ii}=0$, $w_{ij}neqw_{ji}$, and $n\sqrt{\sigma_i}\sqrt{\sigma_j}>\sum_{i=1}^n(w_{ij}+w_{ji})>0$.

