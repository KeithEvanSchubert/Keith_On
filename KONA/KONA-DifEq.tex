\chapter{Differential Equations}\label{c-DifEq}Most practical problems will be described by a differential equation.We will not in general know the form of the solution, but we usuallycan find how they change with respect to each other.  From this basiswe would like to be able to find the actual solution.\section{General Introduction}Consider the general differential equation\beq\dot y(x)=f(x,y(x)). \label{DifEq-e1}\eeqUsing basic calculus we see that the solution is given by\beqny(x)=\int f(x,y(x))dx+c.\eeqnOften this is not very useful in solving equations however.  The wayto get practical solutions for this problem is covered indifferential equations, so I will just mention a few.\subsection{Existence}The problem~\ref{DifEq-e1} has a solution on an interval $x_{0}\leq x\leq\min(b_{x},c)$ and $y_{0}\leq y\leq b_{y}$ if the function $f$ is continuouson the interval for $c=\frac{\|y-y_{0}\|}{\max_{x,y}\left(\frac{\partial f(x,y)}{\partial y}\right)}$.\bfig{\tt    \setlength{\unitlength}{0.92pt}\begin{picture}(202,139)\thinlines    \put(152,21){$c$}              \put(155,104){\line(0,-1){73}}              \put(40,31){\framebox(134,73){}}              \put(40,31){\dashbox{5}(95,55){}}              \put(135,85){\circle*{4}}              \put(170,21){$b_x$}              \put(28,101){$b_y$}              \put(132,21){$x$}              \put(33,82){$y$}              \put(10,29){$y_0$}              \put(38,12){$x_0$}              \put(23,31){\vector(1,0){169}}              \put(40,23){\vector(0,1){106}}              \put(52,50){$f(x,y(x))$}\end{picture}}\efig{Existence requirements}{DifEq-f1}\section{Euler's Method}Euler was without a doubt one of the greatest mathematical minds to have ever lived.  He did work in almost every area you can imagine including numerical methods.  His method is simple and easy for people to use.  Unfortunately, as the old numerics folk theorem goes, ``that which is good for a person is bad for a computer and vise versa'' Euler's method is unstable numerically\footnote{The forward method actually, the backward method of Euler is stable though still far from a good method.}.  To introduce the method, say we knew a point, say $(x_0,y(x_0))$, we could denote our first point by the y-coordinate.\beqny_{0} & = & y(x_0)\eeqnNow say we wanted to find the value of the function a short distance away, say at $x_0+h$.  It would be reasonable to do a first order taylor approximation.  Noting that $\dot y = f(x,y)$ allows us to write this in simple terms.\beqny_{1} & = & y(x_{0}+h) \\      & = & y_{0}+hf(x_{0},y_{0})\eeqnWe could extend this process to find\beqny_{n} & = & y(x_{0}+nh) \\      & = & y_{n-1}+hf(x_{n-1},y_{n-1}) \\      & = & y_{n-2}+hf(x_{n-2},y_{n-2})+hf(x_{n-2}+h,y_{n-2}+hf(x_{n-2},y_{n-2}))\eeqnNote that the third (last) term involves estimating based on an estimate.  It is here that the problem comes because the errors can build.  It is easier to show the problem by taking the \textit{Z}-Transform of the second line\beqny_{n} & = & y_{n-1}+hf(x_{n-1},y_{n-1}) \\Y & = & z^{-1}Y+hF(z^{-1}X,z^{-1}Y)\eeqnAssuming we are dealing with linear functions (a common assumption) we can pull the $z^{-1}$ out.Error is given by\beqnY(x)-y_{h}(x) & = & hD(x)+{\cal O}(h^{2}) \\D'(x) & = & g(x)D(x)+\frac{1}{2}Y''(x), \qquad D(x_{o})=0 \\g(x) & = & \left.\frac{\partial f(x,z)}{\partial z}\right|_{z=Y(x)}\eeqn\section{Runge-Kutta}While Euler's method is nice and simple, it is far from the best.Higher order Taylor methods can be derived but these requireevaluating multiple derivatives.  Even Richardson's extrapolation haslimits on its abilities.Consider the Taylor series of $y$.\beqny(x+h) & = & y(x)+hy'(x)+\frac{h^{2}}{2}y''(x)+\ldots \\ & = & y(x)+hf(x,y)+\frac{h^{2}}{2}f'(x,y)+\ldots \\ & = & y(x)+hf(x,y)+\frac{h^{2}}{2}\left(f_{x}(x,y)+f_{y}(x,y)f(x,y)\right)        +\ldots \\ & = & y(x)+hf(x,y)+\frac{h^{2}}{2}\left(f_{x}(x,y)+f_{y}(x,y)f(x,y)\right)        +\ldots \\ & = & y(x)+hf(x,y)+ah\left(\frac{h}{2a}f_{x}(x,y)+           \frac{h}{2a}f(x,y)f_{y}(x,y)\right)        +\ldots\eeqnNow we consider the Taylor series in two variables of $f(x,y)$.\beqnf(x+h,y+k) & = & \sum_{n=0}^{\infty}\frac{1}{n!} \left[h\frac{\partial}{\partial x}+k\frac{\partial}{\partial y}\right]^{n}f(x,y) \\ & = & f(x,y) +\left[h\frac{\partial}{\partial x}+k\frac{\partial}{\partial y}\right]f(x,y) + \ldots \\ & = & f(x,y) +hf_{x}(x,y)+kf_{y}(x,y) + \ldots\eeqnRearranging we find\beqnf(x+h,y+k) -f(x,y)  & = &hf_{x}(x,y)+kf_{y}(x,y) + \ldots .\eeqnWe use this in our Taylor series of $y$.\beqny(x+h)  & = & y(x)+hf(x,y)+ah\left(\frac{h}{2a}f_{x}(x,y)+\frac{h}{2a}f(x,y)f_{y}(x,y)\right)        +\ldots \\  & = & y(x)+hf(x,y)+h\left(af(x+\frac{h}{2a},y+\frac{h}{2a}f(x,y)) -af(x,y)\right)        +\ldots \\  & = & y(x)+h(1-a)f(x,y)+ahf(x+\frac{h}{2a},y+\frac{h}{2a}f(x,y))        +\ldots \\  & = & y(x)+h(1-a)f(x,y)+ahf(x+b,y+bf(x,y))        +\ldots \\& & \qquad b=\frac{h}{2a} \qquad 0\leq a \leq 1\eeqnWe have defined this in order to take advantage of the many varietiesof second order R-K.  The most common are: Midpoint (a=1), ModifiedEuler (a=1/2), and Heun's method (a=3/4).You can also do R-K for higher orders.  The most common is fourthorder.  The algebra is tedious so I will just present the result.\beqny(x+h) = y(x)+\frac{1}{6}\left(F_{1}+2F_{2}+2F_{3}+F_{4}\right)\eeqnwith\beqnF_{1} & = & hf(x,y) \\F_{2} & = & hf(x+\frac{h}{2},y+\frac{F_{1}}{2}) \\F_{3} & = & hf(x+\frac{h}{2},y+\frac{F_{2}}{2}) \\F_{4} & = & hf(x+h,y+F_{3})\eeqn\section{Fehlberg's Method}We have seen that the local error (error in one step due mostly totruncation) is one order of magnitude better than the global error,in general.  Often we use Richardson's Error formula to find anestimate of the local error ($T_{n}$) so we can adjust the step sizeto keep things nice.  For instance Richardson's Error for Euler'smethod gives us\beqnY(x)-y_{h}(x) & \approx & hD(x) \\Y(x)-y_{2h}(x) & \approx & 2hD(x) \\Y(x)-y_{2h}(x) & \approx & 2(Y(x)-y_{h}(x)) \\Y(x) & \approx & 2y_{h}(x)-y_{2h}(x) \\Y(x)-y_{h}(x) & \approx & (2y_{h}(x)-y_{2h}(x))-y_{h}(x) \\ & \approx & y_{h}(x)-y_{2h}(x).\eeqnThis gives us a reasonable extrapolation formula, and estimate of theerror.  Another way to estimate the error would be to look at twoestimates from different order methods.  For instance you could do afourth and a fifth order R-K estimate at each step and use thedifference to bound the error.  If the error at any step became tolarge then you would decrease the step size and try again.  This ideais Fehlberg's method, and it is the basis of most modern ode solvers.This is why Matlab has ode23 and ode45.\section{Adams-Bashforth}Up till now we have been looking at solving the differential equationdirectly.  We could however just integrate both sides.\beqny' & = & f(x,y) \\\int_{x_{n}}^{x_{n+1}}y'dx & = & \int_{x_{n}}^{x_{n+1}}f(x,y)dx \\y(x_{n+1})-y(x_{n}) & = & \int_{x_{n}}^{x_{n+1}}f(x,y)dx \\y(x_{n+1}) & = & y(x_{n}) + \int_{x_{n}}^{x_{n+1}}f(x,y)dx\eeqnOur task is now reduced to trying to find the remaining integral,which we can use the ideas we had from last chapter.  Adams-Bashforthof order $m$ uses a polynomial approximation to $f(x,y)$ at the points$x_{n}$, $x_{n-1}$, \ldots, $x_{n-m+1}$.  Consider the second orderAdams-Bashforth method.\beqnf(x,y(x)) & \approx &  \frac{x_{n}-x}{h}f(x_{n-1})+\frac{x-x_{n-1}}{h}f(x_{n}) \\\int_{x_{n}}^{x_{n+1}}f(x,y)dx & \approx &  \int_{x_{n}}^{x_{n+1}}\left(  \frac{x_{n}-x}{h}f(x_{n-1})+\frac{x-x_{n-1}}{h}f(x_{n})  \right)dx \\  & \approx &  \left.  \frac{x_{n}x-\frac{1}{2}x^{2}}{h}f(x_{n-1})+  \frac{\frac{1}{2}x^{2}-x_{n-1}x}{h}f(x_{n})  \right|_{x_{n}}^{x_{n+1}} \\  & \approx &  \frac{x_{n}h-\frac{x_{n+1}^{2}-x_{n}^{2}}{2}}{h}f(x_{n-1})+  \frac{\frac{x_{n+1}^{2}-x_{n}^{2}}{2}-x_{n-1}h}{h}f(x_{n}) \\  & \approx &  \frac{x_{n}h-\frac{x_{n+1}^{2}-x_{n+1}x_{n}+x_{n+1}x_{n}-x_{n}^{2}}{2}}{h}f(x_{n-1}) \\  & & \qquad +  \frac{\frac{x_{n+1}^{2}-x_{n+1}x_{n}+x_{n+1}x_{n}-x_{n}^{2}}{2}-x_{n-1}h}{h}f(x_{n}) \\  & \approx &  \frac{x_{n}h-\frac{x_{n+1}h+x_{n}h}{2}}{h}f(x_{n-1})+  \frac{\frac{x_{n+1}h+x_{n}h}{2}-x_{n-1}h}{h}f(x_{n}) \\  & \approx &  \frac{2x_{n}-(x_{n+1}+x_{n})}{2}f(x_{n-1})+  \frac{x_{n+1}+x_{n}-2x_{n-1}}{2}f(x_{n}) \\  & \approx &  \frac{x_{n}-x_{n+1}}{2}f(x_{n-1})+  \frac{x_{n+1}-x_{n-1}+x_{n}-x_{n-1}}{2}f(x_{n}) \\  & \approx &  \frac{-h}{2}f(x_{n-1})+\frac{2h+h}{2}f(x_{n}) \\  & \approx &  \frac{h}{2}(3f(x_{n})-f(x_{n-1}))\eeqnThis is kind of ugly, so it would be nice to have a faster way ofhandling things, especially as the dimensions increase.  Luckily thereis just such a technique.  The method of undetermined coefficients.  We start by assuming the general form we want, in this case,\beqn\int_{x_{n}}^{x_{n+1}}f(x,y)dx & \approx & af(x_n)+bf(x_{n-1}\eeqnWe would like the approximation to work perfectly for constant and linear terms so:Constant term, $f(x,y)=1$\beqn\int_{x_{n}}^{x_{n+1}}1dx & = & a\cdot 1+b\cdot 1 \\h &=& a+b.\eeqnLinear term, $f(x,y)=x$\beqn\int_{x_{n}}^{x_{n+1}}xdx & = & a\cdot x_n+b\cdot x_{n-1} \\\frac{x_{n+1}^2-x_n^2}{2} &=& a\cdot x_n-a\cdot x_{n-1}+a\cdot x_{n-1}+b\cdot x_{n-1} \\\frac{x_{n+1}^2-x_nx_{n+1}+x_nx_{n+1}-x_n^2}{2} &=& a\cdot(x_n-x_{n-1})+(a+b)\cdot x_{n-1}.\eeqnNow noting that $a+b=h$\beqn\frac{x_{n+1}(x_{n+1}-x_n)+x_n(x_{n+1}-x_n)}{2} &=& a\cdot(h)+(h)\cdot x_{n-1} \\\frac{x_{n+1}h+x_nh}{2} &=& a\cdot h+h\cdot x_{n-1} \\\frac{x_{n+1}+x_n-2x_{n-1}}{2} &=& a \\\frac{x_{n+1}-x_{n-1}+x_n-x_{n-1}}{2} &=& a \\\frac{2h+h}{2} &=& a \\\frac{3h}{2} &=& a.\eeqnThus since $a+b=h$,\beqnb=\frac{-h}{2}\eeqnwhich is what we found before.\section{Adams-Moulton}Adams-Bashforth considered that we knew only up to $f(x,y)$ only at points up to $x_n$.  What if we assume we can use $x_n$ or a near approximation?  Let us again consider just the simple case of linear approximations to function, though we could use any order of polynomial we liked (if we want to do the work).\beqnf(x,y(x)) & \approx &  \frac{x-x_{n}}{h}f(x_{n+1})+\frac{x_{n+1}-x}{h}f(x_{n})\eeqnUsing this we can solve the integral.\beqn\int_{x_{n}}^{x_{n+1}}f(x,y)dx & \approx &  \int_{x_{n}}^{x_{n+1}}\left(  \frac{x-x_{n}}{h}f(x_{n+1})+\frac{x_{n+1}-x}{h}f(x_{n})  \right)dx \\  & \approx &  \left.  \frac{\frac{1}{2}x^{2}}{h}f(x_{n+1}-x_{n}x)+  \frac{x_{n+1}x-\frac{1}{2}x^{2}}{h}f(x_{n})  \right|_{x_{n}}^{x_{n+1}} \\  & \approx &  \frac{\frac{x_{n+1}^{2}-x_n^{2}}{2}-x_nx_{n+1}+x_n^2}{h}f(x_{n+1})+  \frac{x_{n+1}^2-x_{n+1}x_n-\frac{x_{n+1}^{2}-x_n^{2}}{2}}{h}f(x_{n}) \\  & \approx &  \frac{x_{n+1}^{2}-x_n^{2}-2x_nx_{n+1}+2x_n^2}{2h}f(x_{n+1})+  \frac{2x_{n+1}^2-2x_{n+1}x_n-x_{n+1}^{2}+x_n^{2}}{2h}f(x_{n}) \\  & \approx &  \frac{x_{n+1}^{2}-2x_nx_{n+1}+x_n^2}{2h}f(x_{n+1})+  \frac{x_{n+1}^2-2x_{n+1}x_n-+x_n^{2}}{2h}f(x_{n}) \\  & \approx &  \frac{(x_{n+1}-x_n)^2}{2h}f(x_{n+1})+  \frac{(x_{n+1}-x_n)^2}{2h}f(x_{n}) \\  & \approx &  \frac{h^2}{2h}f(x_{n+1})+\frac{h^2}{2h}f(x_{n}) \\  & \approx &  \frac{h}{2}(f(x_{n+1})+f(x_{n}))\eeqn\section{Stability \& Stiff Equations}A good start for looking at stiff equations is to examine thestability of our methods.  Consider Forward Euler for\beqny'=-200y, \qquad y(1)=e^{-200}.\eeqnThe solution can easily be seen to be $e^{-200x}$.  Forward Eulergives us\beqny_{i+1} & = & y_{i}-200*h*y_{i} \\ & = & y_{i}(1-200*h).\eeqnWe note that this is stable if $\| 1-200*h \|<1$.  Since $h>0$ wemust have $h<0.01$.  This is a smooth, monotonically decreasingfunction that is less than $2\time10^{-87}$ and greater than zero.Despite the smoothness and flatness, we have to take very smallsteps.  To see this look at Figure~\ref{stabfig}.\begin{figure}[h]\begin{center}\leavevmode\hbox{\epsfxsize=4in\epsffile{stab.eps}}\end{center}\caption{Instability in Euler's Method}\label{stabfig}\end{figure}Note that Backward Euler does not have the same problem.  It isdefined by\beqny_{i+1} & = & y_{i}-200*h*y_{i+1} \\ & = & y_{i}\frac{1}{1+200*h},\eeqnwhich is stable for all $h>0$. Stability is not the same thing asstiffness, but they are related.  Stiffness is due to multiple scalesof the terms, for instance $e^{-x}$, $e^{-200x}$.  Consider thefollowing equation\beqn0=\ddot{y}+1001\dot{y}+1000y, \qquad y(0)=1, \qquad \dot{y}(0)=-1.\eeqnThe solution can easily be seen to be $e^{-t}$, which is nice in anydefinition.  We solve the equation using a 4th order R-K method.  Theresults are in Figure~\ref{stifffig}.\begin{figure}[h]\begin{center}\leavevmode\hbox{\epsfxsize=6in\epsffile{stiff.eps}}\end{center}\caption{Stiff Equation}\label{stifffig}\end{figure}What is going on?  We need to look at the eigenvalues.Define the intermediate variable $z=\dot y$, and the equation becomes\beqn0&=&\dot z +1001z+1000y \\\dot z&=&-1001z-1000y.\eeqnPutting this in matrix form\beqn\dot{\bmat y\\z \emat} &=&\bmat 0 & 1 \\ -1000 & -1001 \emat \bmat y\\z\emat\eeqnThe eigenvalues are (-1,-1000).  Since the eigenvalues differ by three orders of magnitude we can expect stiffness, which is what Figure~\ref{stifffig} shows.