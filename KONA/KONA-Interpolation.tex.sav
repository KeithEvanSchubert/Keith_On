\chapter{Interpolation}\label{c-IntApp}We will now look at the problem of finding a polynomial to fit a setof points.  The points could come from measurements in an experiment,or it could come from a complex function we want to approximate.  Ineither case we will begin by considering the case where we want ourpolynomial to be exact at these values.  An obvious question is whythe emphasis on polynomials, when so many other functions exist.Indeed we do see the use of other basis (sin and cos in Fourier forexample), but still polynomials hold a special place in manyapplications.  One major reason is the Theorem of Wiestrass from RealAnalysis.  It basically says that polynomials can approximate anyfunction (assuming you use the entire basis).\section{Lagrange Interpolation Basis}Probably the nicest way to visualize the interpolation polynomials isto consider the Lagrange interpolation basis functions.  For the setof points, $\{x_{0}, x_{1}, \ldots, x_{n}\}$ define the following polynomial:\beqnL_{i}(x)=\frac{\prod_{j\ne i}(x-x_{j})}{\prod_{j\ne i}(x_{i}-x_{j})}.\eeqnWe note in particular that $L_{i}(x_{j})=\delta_{i,j}$, which allowsus to get the interpolation polynomial nicely.  The interpolating polynomialis then given by\beqnP_{n}(x)=\sum_{i=0}^{n}y_{i}L_{i}(x).\eeqnThe importance of the Lagrange basis giving us the Kronecker deltafunction cannot be over-emphasized, as it is the essential idea ingetting the solution.Often the points are selected to be evenly spaced due to constraintsin the basic system.  While this is not the best for errors, it isoften a physical necessity (for example many data samplers areconstrained this way).  In this case we can simplify the expressionusing\beqn\mu=\frac{x-x_{0}}{x_{1}-x_{0}}.\eeqnThis is covered well in the book.\section{Divided Difference}Divided difference is a similar method to Taylor approximation butinstead of matching derivatives exactly at a point, it nearlyapproximates the derivative to exactly match certain points.  Theresult is the same as Lagrange's formula.  We will start our derivation of the formula by considering the Taylor approximation around the point $x_0$.\beqnp_k(x) &=& \sum_{i=0}^{k}\frac{(x-x_0)^i}{i!}f^{(i)}(x_0)\eeqnThe simplest case is when $k=0$, in which case we have a horizontal line through $f(x_0)$.\beqnp_0(x) &=& \frac{(x-x_0)^0}{0!}f^{(0)}(x_0) \\       &=& \frac{1}{1}f(x_0) \\       &=& f(x_0)\eeqnThis is very easy to convert into a one point interpolation formula, as it is already one.  I will use capitals for the divided difference formula.\beqnP_0(x) &=& f(x_0)\eeqnNow let's consider the case when $k=1$, in which case we have a line tangent to the curve through the point $(x_0,f(x_0))$.\beqnp_1(x) &=& \frac{(x-x_0)^0}{0!}f^{(0)}(x_0)+\frac{(x-x_0)^1}{1!}f^{(1)}(x_0) \\       &=& f(x_0)+(x-x_0)f^{(1)}(x_0)\eeqnTo convert this we have to consider how to discritize the derivative.  The first derivative is\beqnf^{(1)}(x) &=& \frac{d}{dx}f(x) \\        &=& \lim_{x_1\rightarrow x}\frac{f(x_{1})-f(x)}{x_{1}-x}\eeqnWe can approximate this by not allowing $x_1$ to go to $x$ (i.e. remove the limit), then by noting we are consider the derivative at the point $x=x_0$, we have a neat expression.\beqnf^{(1)}(x) &\approx& F[x_{0},x_{1}] \\           & = & \frac{f(x_{1})-f(x_{0})}{x_{1}-x_{0}}\eeqnPutting this back into the expression we have the second divided difference formula\beqnP_1(x) &=& f(x_0)+(x-x_0)F[x_0,x_1] \\       &=& f(x_0)+(x-x_0)\frac{f(x_1)-f(x_0)}{x_1-x_0}\eeqnJust to show that this is the same as the first Lagrange interpolator\beqnP_1(x) &=& f(x_0)+(x-x_0)\frac{f(x_1)-f(x_0)}{x_1-x_0} \\       &=& f(x_0)\frac{x_1-x_0}{x_1-x_0}+(f(x_1)-f(x_0))\frac{x-x_0}{x_1-x_0} \\       &=& f(x_0)\frac{x_1-x_0}{x_1-x_0}+f(x_1)\frac{x-x_0}{x_1-x_0}-f(x_0)\frac{x-x_0}{x_1-x_0} \\       &=& f(x_0)\frac{x_1-x_0}{x_1-x_0}-f(x_0)\frac{x-x_0}{x_1-x_0}+f(x_1)\frac{x-x_0}{x_1-x_0} \\       &=& f(x_0)\frac{x_1-x}{x_1-x_0}+f(x_1)\frac{x-x_0}{x_1-x_0}\eeqnThe final line is Lagrange's interpolator for two points.Let's do one more step, then I will show the general solution.\beqnF[x_{0},x_{1},\cdots,x_{n}] & = & \frac{F[x_{1},\cdots,x_{n}]-F[x_{0},\cdots,x_{n-1}]}{x_{n}-x_{0}}\eeqn\beqnp_k(x) &=& f(x_0)+ \sum_{i=1}^{k}\frac{(x-x_0)}{i!}\frac{d^i}{dx^i}f(x) \\P_{1}(x) & = & f(x_{0})+(x-x_{0})F[x_{0},x_{1}] \\P_{k+1} & = &P_{k}+(x-x_{0})(x-x_{1})\cdots(x-x_{k})F[x_{0},x_{1},\cdots,x_{k+1}]\eeqnHomework:Section 5.1: 5, 9, 13Section 5.2: 2, 3, 7\section{Error}The key area to note from here is that the error is given by eitherof the following formulas.\beqnf(x)-P_{n}(x) & = &\prod_{i=0}^{n}(x-x_{i})\frac{f^{(n+1)}(c_{x})}{(n+1)!} \\ & = &\prod_{i=0}^{n}(x-x_{i})F[x_{0},x_{1},\cdots,x_{n},x]\eeqnThe important part of this is to note that these are themselvespolynomials of order $n+1$.  Consider the plot of a polynomial withequi-spaced roots.  It is trivial to note that the height of the peaksbetween the roots is bigger towards the outside of the interval.\section{Splines}For splines we want to fit a cubic polynomial for each interval sothat the first and second derivatives between two sections match onthe boundary.  Following the books derivation we get the formula forthe polynomial on the interval $[x_{j-1},x_{j}]$ to be\beqns(x) & = &       a_{1}(x_{j}-x)^{3}+a_{0}(x-x_{j-1})^{3}      +b_{1}(x_{j}-x)+b_{0}(x-x_{j-1}) \\a_{i} & = & \frac{M_{j-i}}{6(x_{j}-x_{j-1})} \\b_{i} & = & \frac{y_{j-i}-\frac{1}{6}M_{j-i}(x_{j}-x_{j-1})^{2}}{(x_{j}-x_{j-1})}\eeqnThe only thing that we need is to calculate $M_{i}$ for the naturalcubic spline, which is done byrequiring $M_{1}=M_{n}=0$ and solving the following matrix system\beqnAx & = & b \\A & = &\left[\begin{matrix}\alpha_{2} & \beta_{2}  &  0        & \cdots       & 0 \cr\beta_{2}  & \alpha_{3} & \beta_{3} & \ddots       & \vdots \cr0          & \beta_{3}  & \ddots    & \ddots       & 0 \cr\vdots     & \ddots     & \ddots    & \alpha_{n-2} & \beta_{n-2} \cr0          & \cdots     & 0         & \beta_{n-2}  & \alpha_{n-1}\end{matrix}\right] \\x & = &\left[\begin{matrix}M_{2} \cr\vdots \crM_{n-1}\end{matrix}\right] \qquadb =\left[\begin{matrix}\gamma_{2}-\gamma_{1} \cr\vdots \cr\gamma_{n-1}-\gamma_{n-2}\end{matrix}\right] \\\alpha_{i} & = & \frac{x_{i+1}-x_{i-1}}{3}\qquad\beta_{i} = \frac{x_{i+1}-x_{i}}{6}\qquad\gamma_{i} = \frac{y_{i+1}-y_{i}}{x_{i+1}-x_{i}}\eeqnWe can also find the $M_{i}$ for the not-a-knot cubic spline, which isoften preferred by solving a similar system\beqnAx & = & b \\A & = &\left[\begin{matrix}\psi_{1}   & \beta_{1}  &  0         &  0         & \cdots       & 0            & 0\cr\beta_{1}  & \alpha_{2} & \beta_{2}  &  0         & \cdots       & 0            & 0 \cr0          & \beta_{2}  & \alpha_{3} & \beta_{3}  & \ddots       & \vdots       & \vdots \cr0          & 0          & \beta_{3}  & \ddots     & \ddots       & 0            & 0 \cr\vdots     & \vdots     & \ddots     & \ddots     & \alpha_{n-2} & \beta_{n-2}  & 0 \cr0          & 0          & \cdots     & 0          & \beta_{n-2}  & \alpha_{n-1} & \beta_{n-1} \cr0          & 0          & \cdots     & 0          & 0            & \beta_{n-1}  & \phi_{2}\end{matrix}\right] \\x & = &\left[\begin{matrix}M_{1} \crM_{2} \cr\vdots \crM_{n-1} \crM_{n}\end{matrix}\right] \qquadb =\left[\begin{matrix}\gamma_{1}-f'(x_{1}) \cr\gamma_{2}-\gamma_{1} \cr\vdots \cr\gamma_{n-1}-\gamma_{n-2} \crf'(x_{n})-\gamma_{n-1}\end{matrix}\right] \\\alpha_{i} & = & \frac{x_{i+1}-x_{i-1}}{3}\qquad\beta_{i} = \frac{x_{i+1}-x_{i}}{6}\qquad\gamma_{i} = \frac{y_{i+1}-y_{i}}{x_{i+1}-x_{i}} \\\psi_{1} & = & \frac{x_{2}-x_{1}}{3}\qquad\phi_{2} = \frac{x_{n}-x_{n-1}}{3}\eeqnor (if you don't know the derivative)\beqnAx & = & b \\A & = &\left[\begin{matrix}\psi_{1}   & \psi_{2}   &  0         &  0         & \cdots       & 0            & 0 \cr\beta_{1}  & \alpha_{2} & \beta_{2}  &  0         & \cdots       & 0            & 0 \cr0          & \beta_{2}  & \alpha_{3} & \beta_{3}  & \ddots       & \vdots       & \vdots \cr0          & 0          & \beta_{3}  & \ddots     & \ddots       & 0            & 0 \cr\vdots     & \vdots     & \ddots     & \ddots     & \alpha_{n-2} & \beta_{n-2}  & 0 \cr0          & 0          & \cdots     & 0          & \beta_{n-2}  & \alpha_{n-1} & \beta_{n-1} \cr0          & 0          & \cdots     & 0          & 0            & \phi_{2}     & \phi_{1}\end{matrix}\right] \\x & = &\left[\begin{matrix}M_{1} \crM_{2} \cr\vdots \crM_{n-1} \crM_{n}\end{matrix}\right] \qquadb =\left[\begin{matrix}\psi_{3} \cr\gamma_{2}-\gamma_{1} \cr\vdots \cr\gamma_{n-1}-\gamma_{n-2} \cr\phi_{3}\end{matrix}\right] \\\alpha_{i} & = & \frac{x_{i+1}-x_{i-1}}{3}\qquad\beta_{i} = \frac{x_{i+1}-x_{i}}{6}\qquad\gamma_{i} = \frac{y_{i+1}-y_{i}}{x_{i+1}-x_{i}} \\\xi_{1} & = & x_{2}-x_{1}\qquad\xi_{2} = x_{2}-z_{1}\qquad\xi_{3} = z_{1}-x_{1} \\\psi_{1} & = & \frac{\xi_{2}^{3}-\xi_{1}^{2}\xi_{2}}{6\xi_{1}}\qquad\psi_{2} = \frac{\xi_{3}^{3}-\xi_{1}^{2}\xi_{3}}{6\xi_{1}}\qquad\psi_{3} = f(z_{1})-\frac{\xi_{2}y_{1}+\xi_{3}y_{2}}{\xi_{1}} \\\xi_{4} & = & x_{n}-x_{n-1}\qquad\xi_{5} = x_{n}-z_{2}\qquad\xi_{6} = z_{2}-x_{n-1} \\\phi_{1} & = & \frac{\xi_{5}^{3}-\xi_{4}^{2}\xi_{5}}{6\xi_{4}}\qquad\phi_{2} = \frac{\xi_{6}^{3}-\xi_{4}^{2}\xi_{6}}{6\xi_{4}}\qquad\phi_{3} = f(z_{2})-\frac{\xi_{5}y_{n-1}+\xi_{6}y_{n}}{\xi_{4}}\eeqnNote, you can easily enter the matrix $A$ into Matlab by using thecommand diag.  For instance, if you put the entries of $A$ that are onthe main diagonal into the vector $A1$, the first sub-diagonal into$A2$, and the first super-diagonal into $A3$, then in Matlab you enter,{\it A=diag(A1)+diag(A2,-1)+diag(A3,1);}.Homeworksection 5.3: 7section 5.4: 3, 5