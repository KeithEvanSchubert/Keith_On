\chapter{Interpolation}\label{c-IntApp}We will now look at the problem of finding a polynomial to fit a setof points.  The points could come from measurements in an experiment,or it could come from a complex function we want to approximate.  Ineither case we will begin by considering the case where we want ourpolynomial to be exact at these values.  An obvious question is whythe emphasis on polynomials, when so many other functions exist.Indeed we do see the use of other basis (sin and cos in Fourier forexample), but still polynomials hold a special place in manyapplications.  One major reason is the Theorem of Wiestrass from RealAnalysis.  It basically says that polynomials can approximate anyfunction (assuming you use the entire basis).\section{Lagrange Interpolation Basis}Probably the nicest way to visualize the interpolation polynomials isto consider the Lagrange interpolation basis functions.  For the setof points, $\{x_{0}, x_{1}, \ldots, x_{n}\}$ define the following polynomial:\beqnL_{i}(x)=\frac{\prod_{j\ne i}(x-x_{j})}{\prod_{j\ne i}(x_{i}-x_{j})}.\eeqnWe note in particular that $L_{i}(x_{j})=\delta_{i,j}$, which allowsus to get the interpolation polynomial nicely.  The interpolating polynomialis then given by\beqnP_{n}(x)=\sum_{i=0}^{n}y_{i}L_{i}(x).\eeqnThe importance of the Lagrange basis giving us the Kronecker deltafunction cannot be over-emphasized, as it is the essential idea ingetting the solution.Often the points are selected to be evenly spaced due to constraintsin the basic system.  While this is not the best for errors, it isoften a physical necessity (for example many data samplers areconstrained this way).  In this case we can simplify the expressionusing\beqn\mu=\frac{x-x_{0}}{x_{1}-x_{0}}.\eeqnThis is covered well in the book.\section{Newton's Divided Difference}Newton's divided difference is a similar method to Taylor approximation butinstead of matching derivatives exactly at a point, it nearlyapproximates the derivative to exactly match certain points.  They are the same if the interpolation points are made to coincide.  The result is the same as Lagrange's formula.  We will start our derivation of the formula by considering the Taylor approximation around the point $x_0$.\beqnp_k(x) &=& \sum_{i=0}^{k}\frac{(x-x_0)^i}{i!}f^{(i)}(x_0)\eeqnThe simplest case is when $k=0$, in which case we have a horizontal line through $f(x_0)$.\beqnp_0(x) &=& \frac{(x-x_0)^0}{0!}f^{(0)}(x_0) \\       &=& \frac{1}{1}f(x_0) \\       &=& f(x_0)\eeqnThis is very easy to convert into a one point interpolation formula, as it is already one.  I will use capitals for the divided difference formula.\beqnP_0(x) &=& f(x_0)\eeqnNow let's consider the case when $k=1$, in which case we have a line tangent to the curve through the point $(x_0,f(x_0))$.\beqnp_1(x) &=& \frac{(x-x_0)^0}{0!}f^{(0)}(x_0)+\frac{(x-x_0)^1}{1!}f^{(1)}(x_0) \\       &=& f(x_0)+(x-x_0)f^{(1)}(x_0)\eeqnTo convert this we have to consider how to discritize the derivative.  The first derivative is\beqnf^{(1)}(x) &=& \frac{d}{dx}f(x) \\        &=& \lim_{x_1\rightarrow x}\frac{f(x_{1})-f(x)}{x_{1}-x}\eeqnWe can approximate this by not allowing $x_1$ to go to $x$ (i.e. remove the limit), then by noting we are consider the derivative at the point $x=x_0$, we have a neat expression.\beqnf^{(1)}(x) &\approx& F[x_{0},x_{1}] \\           & = & \frac{f(x_{1})-f(x_{0})}{x_{1}-x_{0}}\eeqnPutting this back into the expression we have the second divided difference formula\beqnP_1(x) &=& f(x_0)+(x-x_0)F[x_0,x_1] \\       &=& f(x_0)+(x-x_0)\frac{f(x_1)-f(x_0)}{x_1-x_0} \\       &=& P_0(x)+(x-x_0)\frac{f(x_1)-f(x_0)}{x_1-x_0}\eeqnJust to show that this is the same as the first Lagrange interpolator\beqnP_1(x) &=& f(x_0)+(x-x_0)\frac{f(x_1)-f(x_0)}{x_1-x_0} \\       &=& f(x_0)\frac{x_1-x_0}{x_1-x_0}+(f(x_1)-f(x_0))\frac{x-x_0}{x_1-x_0} \\       &=& f(x_0)\frac{x_1-x_0}{x_1-x_0}+f(x_1)\frac{x-x_0}{x_1-x_0}-f(x_0)\frac{x-x_0}{x_1-x_0} \\       &=& f(x_0)\frac{x_1-x_0}{x_1-x_0}-f(x_0)\frac{x-x_0}{x_1-x_0}+f(x_1)\frac{x-x_0}{x_1-x_0} \\       &=& f(x_0)\frac{x_1-x}{x_1-x_0}+f(x_1)\frac{x-x_0}{x_1-x_0}\eeqnThe final line is Lagrange's interpolator for two points.Let's do one more step, then I will show the general solution.  The reason for the extra step is it shows the difference between Taylor and Newton.\beqnp_2(x) &=& \frac{(x-x_0)^0}{0!}f^{(0)}(x_0)+\frac{(x-x_0)^1}{1!}f^{(1)}(x_0)+\frac{(x-x_0)^2}{2!}f^{(2)}(x_0) \\       &=& f(x_0)+(x-x_0)f^{(1)}(x_0)+(x-x_0)^2\frac{f^{(2)}(x_0)}{2}\eeqnBefore we do anything else, we have to unwind the $(x-x_0)^2$ term.  Why you may ask?  Is it not already in a usable form?  Well yes and no.  As I stated in the beginning, Taylor assumed coinciding points, but Newton assumed different points to be interpolated.  Up until now this has not been an issue.  This is the first challenge.  Newton used basis polynomials of the form $n_k=\prod_{i=0}^{k-1}(x-x_i)$, so we have to use $(x-x_0)(x-x_1)$ for this case instead of $(x-x_0)^2$.  Note that in general we will replace $(x-x_0)^k$ by $\prod_{i=0}^{k-1}(x-x_i)$.Now on to the derivative term.  It might seem odd to combine the factorial term with the derivative, but it has practicality and some intuition on its side.  Let me first give the formula.\beqn\frac{f^{(2)}(x_0)}{2}&=&F[x_{0},x_{1},x_{2}] \\ & = & \frac{F[x_1,x_2]-F[x_0,x_1]}{x_2-x_0} \\ & = & \frac{\frac{f(x_{2})-f(x_{1})}{x_{2}-x_{1}}-\frac{f(x_{1})-f(x_{0})}{x_{1}-x_{0}}}{x_2-x_0}\eeqnAt this point I can explain how the coll If the points are equidistant, then this makes lots of since as the denominator of the final term is then twice the step size.  When you would go to the third derivative you would have the difference of two second derivatives that would have the one half scaling built in, divided by a length that was three times the size so this would have to have a factor of three in the denominator.  The one third from the length together with the one half from the second derivative gives a factor of one over three factorial.  Since the next term up would use the previous term, but have a larger denominator, we have an induction step\footnote{This is probably not obvious at this point, but I have left it this way to give you a challenge.  Sorry but trying is the only way to learn.  Try to write the recursion, and drop by or send an email if you don't get it.}Putting this back into the expression we have the second divided difference formula\beqnP_2(x) &=& f(x_0)+(x-x_0)F[x_0,x_1]+(x-x_0)(x-x_1)F[x_{0},x_{1},x_{2}] \\       &=& f(x_0)+(x-x_0)\frac{f(x_1)-f(x_0)}{x_1-x_0}+(x-x_0)(x-x_1)\frac{\frac{f(x_{2})-f(x_{1})}{x_{2}-x_{1}}-\frac{f(x_{1})-f(x_{0})}{x_{1}-x_{0}}}{x_2-x_0} \\       &=& P_1(x)+(x-x_0)(x-x_1)F[x_{0},x_{1},x_{2}] \\       &=& P_1(x)+(x-x_0)(x-x_1)\frac{\frac{f(x_{2})-f(x_{1})}{x_{2}-x_{1}}-\frac{f(x_{1})-f(x_{0})}{x_{1}-x_{0}}}{x_2-x_0}\eeqnJust to show that this is the same as the second Lagrange interpolator\beqnP_2(x) &=& P_1(x) + (x-x_0)(x-x_1)(x-x_0)(x-x_1)\frac{\frac{f(x_{2})-f(x_{1})}{x_{2}-x_{1}}-\frac{f(x_{1})-f(x_{0})}{x_{1}-x_{0}}}{x_2-x_0} \\ &=& f(x_0)\frac{x_1-x}{x_1-x_0}+f(x_1)\frac{x-x_0}{x_1-x_0} \\ &&\quad +(x-x_0)(x-x_1)\frac{\frac{f(x_{2})-f(x_{1})}{x_{2}-x_{1}}-\frac{f(x_{1})-f(x_{0})}{x_{1}-x_{0}}}{x_2-x_0} \\ &=& f(x_0)\frac{x_1-x}{x_1-x_0}+f(x_1)\frac{x-x_0}{x_1-x_0} \\ &&\quad +(x-x_0)(x-x_1)\left(\frac{f(x_{2})-f(x_{1})}{(x_2-x_1)(x_2-x_0)}-\frac{f(x_{1})-f(x_{0})}{(x_1-x_0)(x_2-x_0)}\right) \\ &=& f(x_0)\frac{x_1-x}{x_1-x_0}+f(x_1)\frac{x-x_0}{x_1-x_0} \\ &&\quad +(f(x_{2})-f(x_{1}))\frac{(x-x_0)(x-x_1)}{(x_2-x_0)(x_2-x_1)} -(f(x_{1})-f(x_{0}))\frac{(x-x_0)(x-x_1)}{(x_2-x_0)(x_1-x_0)} \\ &=& f(x_0)\frac{x_1-x}{x_1-x_0}+f(x_{0})\frac{(x-x_0)(x-x_1)}{(x_2-x_0)(x_1-x_0)} \\ &&\quad +f(x_1)\frac{x-x_0}{x_1-x_0}-f(x_{1})\frac{(x-x_0)(x-x_1)}{(x_2-x_0)(x_2-x_1)} -f(x_{1})\frac{(x-x_0)(x-x_1)}{(x_2-x_0)(x_1-x_0)} \\ &&\quad +f(x_{2})\frac{(x-x_0)(x-x_1)}{(x_2-x_0)(x_2-x_1)}\eeqn\beqnP_2(x) &=& f(x_0)\left(\frac{(x_0-x_2)(x-x_1)}{(x_0-x_2)(x_0-x_1)}+\frac{(x-x_0)(x-x_1)}{(x_0-x_2)(x_0-x_1)}\right) \\ &&\quad +f(x_1)\frac{x-x_0}{x_1-x_0}-f(x_{1})\frac{(x-x_0)(x-x_1)}{x_2-x_0}\left(\frac{1}{x_2-x_1}+ \frac{1}{x_1-x_0}\right) \\ &&\quad +f(x_{2})\frac{(x-x_0)(x-x_1)}{(x_2-x_0)(x_2-x_1)} \\  &=& f(x_0)\frac{(x-x_0+x_0-x_2)(x-x_1)}{(x_0-x_2)(x_0-x_1)} \\ &&\quad +f(x_1)\frac{x-x_0}{x_1-x_0}-f(x_{1})\frac{(x-x_0)(x-x_1)}{x_2-x_0}\left(\frac{x_1-x_0+x_2-x_1}{(x_2-x_1)(x_1-x_0)}\right) \\ &&\quad +f(x_{2})\frac{(x-x_0)(x-x_1)}{(x_2-x_0)(x_2-x_1)} \\  &=& f(x_0)\frac{(x-x_2)(x-x_1)}{(x_0-x_2)(x_0-x_1)} \\ &&\quad +f(x_1)\frac{x-x_0}{x_1-x_0}-f(x_{1})\frac{(x-x_0)(x-x_1)}{x_2-x_0}\left(\frac{x_2-x_0}{(x_2-x_1)(x_1-x_0)}\right) \\ &&\quad +f(x_{2})\frac{(x-x_0)(x-x_1)}{(x_2-x_0)(x_2-x_1)} \\  &=& f(x_0)\frac{(x-x_2)(x-x_1)}{(x_0-x_2)(x_0-x_1)} \\ &&\quad +f(x_1)\frac{(x_1-x_2)(x-x_0)}{(x_1-x_2)(x_1-x_0)}+f(x_{1})\frac{(x-x_0)(x-x_1)}{(x_1-x_2)(x_1-x_0)} \\ &&\quad +f(x_{2})\frac{(x-x_0)(x-x_1)}{(x_2-x_0)(x_2-x_1)} \\  &=& f(x_0)\frac{(x-x_2)(x-x_1)}{(x_0-x_2)(x_0-x_1)} \\ &&\quad +f(x_1)\frac{(x-x_1+x_1-x_2)(x-x_0)}{(x_1-x_2)(x_1-x_0)} \\ &&\quad +f(x_{2})\frac{(x-x_0)(x-x_1)}{(x_2-x_0)(x_2-x_1)} \\  &=& f(x_0)\frac{(x-x_2)(x-x_1)}{(x_0-x_2)(x_0-x_1)}  +f(x_1)\frac{(x-x_2)(x-x_0)}{(x_1-x_2)(x_1-x_0)} +f(x_{2})\frac{(x-x_0)(x-x_1)}{(x_2-x_0)(x_2-x_1)}\eeqnThe last line is the second Lagrange polynomial.  It took a little algebra, but I think it is worthwhile to try a\subsection{General Form}Derivatives\beqnF[x_{0},x_{1}] & = & \frac{f(x_{1})-f(x_{0})}{x_{1}-x_{0}}\\F[x_{0},x_{1},\cdots,x_{n}] & = & \frac{F[x_{1},\cdots,x_{n}]-F[x_{0},\cdots,x_{n-1}]}{x_{n}-x_{0}}\eeqnPolynomial Recursion\beqnP_{0}(x) & = & f(x_{0}) \\P_{k+1} & = &P_{k}+(x-x_{0})(x-x_{1})\cdots(x-x_{k})F[x_{0},x_{1},\cdots,x_{k+1}]\eeqn\subsection{Error}The key area to note from here is that the error is given by eitherof the following formulas.\beqnf(x)-P_{n}(x) & = &\prod_{i=0}^{n}(x-x_{i})\frac{f^{(n+1)}(c_{x})}{(n+1)!} \\ & = &\prod_{i=0}^{n}(x-x_{i})F[x_{0},x_{1},\cdots,x_{n},x]\eeqnThe important part of this is to note that these are themselvespolynomials of order $n+1$.  Consider the plot of a polynomial withequi-spaced roots.  It is trivial to note that the height of the peaksbetween the roots is bigger towards the outside of the interval.\section{Tchebychev Polynomials}Tchebychev (also spelled Chebyshev) polynomials are the basis set of polynomials that reduces the maximum error in interpolation by putting more of the zero crossings to the edges.  The Tchebychev polynomials are defined on the interval $[-1,1]$ by the following recursion\begin{eqnarray}T_0&=&1\\T_1&=&x\\T_n&=&2xT_{n-1}-T_{n-2}\end{eqnarray}The first several Tchebychev polynomials are\begin{eqnarray}T_0(x)&=&1\\T_1(x)&=&x\\T_2(x)&=&2x^2-1\\T_3(x)&=&2x(2x^2-1)-x\\   &=&4x^3-3x\\T_4(x)&=&2x(4x^3-3x)-(2x^2-1)\\   &=&8x^4-8x^2+1\\T_5(x)&=&2x(8x^4-8x^2+1)-(4x^3-3x)\\   &=&16x^5-20x^3+5x\\T_6(x)&=&2x(16x^5-20x^3+5x)-(8x^4-8x^2+1)\\   &=&32x^6-48x^4+18x^2-1\\T_7(x)&=&2x(32x^6-48x^4+18x^2-1)-(16x^5-20x^3+5x)\\   &=&64x^7-112x^5+56x^3-7x\end{eqnarray}The roots of the Tchebychev polynomial of degree $n+1$, $T_{n+1}(x)$, are given by\begin{eqnarray}T_{n+1}(x_k) &=& 0\qquad \iff \\x_k &=& \cos\left(\frac{2n+1-2k}{2n+2}\pi\right)\; \forall k\in[1,\ldots,n]\end{eqnarray}\section{Splines}For splines we want to fit a cubic polynomial for each interval so that the first and second derivatives between two sections match on the boundary. Thus for $n$ points we need $n-1$ spline sections.\begin{eqnarray}S(x)&=&\left\{\begin{matrix}S_0(x) & x_0\leq x\leq x_1 \\S_1(x) & x_1\leq x\leq x_2 \\\vdots & \vdots \\S_i(x) & x_i\leq x\leq x_{i+1} \\\vdots & \vdots \\S_{n-2}(x) & x_{n-2}\leq x\leq x_{n-1} \\\end{matrix}\right.\end{eqnarray}with\footnote{Note that we could have defined this lots of different ways.  For instance, one popular way of developing the same set of equations we will find is to start from\begin{eqnarray}s(x) & = &       a_{1}(x_{j}-x)^{3}+a_{0}(x-x_{j-1})^{3}      +b_{1}(x_{j}-x)+b_{0}(x-x_{j-1})\end{eqnarray}Which would give us (these will make sense in a couple pages)\begin{eqnarray}a_{i} & = & \frac{M_{j-i}}{6(x_{j}-x_{j-1})} \\b_{i} & = & \frac{y_{j-i}-\frac{1}{6}M_{j-i}(x_{j}-x_{j-1})^{2}}{(x_{j}-x_{j-1})}\end{eqnarray}}\begin{eqnarray}S_i(x)&=&a_i(x-x_i)^3+b_i(x-x_i)^2+c_i(x-x_i)+d_i\end{eqnarray}Each spline section has four requirements\begin{enumerate}\item It must go through the boundary point on its left,      \begin{eqnarray}        y_i        &=&S_i(x_i)\\        &=&a_i(x_i-x_i)^3+b_i(x_i-x_i)^2+c_i(x_i-x_i)+d_i\\        y_i&=&d_i      \end{eqnarray}\item It must go through the boundary point on its right,      \begin{eqnarray}        y_{i+1}        &=&S_i(x_{i+1})\\        &=&a_i(x_{i+1}-x_i)^3+b_i(x_{i+1}-x_i)^2+c_i(x_{i+1}-x_i)+d_i\\        &=&a_i(x_{i+1}-x_i)^3+b_i(x_{i+1}-x_i)^2+c_i(x_{i+1}-x_i)+y_i\\        y_{i+1}-y_i&=&a_i(x_{i+1}-x_i)^3+b_i(x_{i+1}-x_i)^2+c_i(x_{i+1}-x_i)\\        \frac{y_{i+1}-y_i}{x_{i+1}-x_i}&=&a_i(x_{i+1}-x_i)^2+b_i(x_{i+1}-x_i)+c_i\\        c_i&=&\frac{y_{i+1}-y_i}{x_{i+1}-x_i}-a_i(x_{i+1}-x_i)^2-b_i(x_{i+1}-x_i)      \end{eqnarray}\item It must have continuous slopes with the splines on either side,      \begin{eqnarray}        \dot S_i(x)        &=&3a_i(x-x_i)^2+2b_i(x-x_i)+c_i      \end{eqnarray}      and thus      \begin{eqnarray}        \dot S_{i-1}(x_i) &=& \dot S_i(x_i) \\        3a_{i-1}(x_i-x_{i-1})^2+2b_{i-1}(x_i-x_{i-1})+c_{i-1}&=&3a_i(x_i-x_i)^2+2b_i(x_i-x_i)+c_i \\        3a_{i-1}(x_i-x_{i-1})^2+2b_{i-1}(x_i-x_{i-1})+c_{i-1}&=&c_i \\        3a_{i-1}(x_i-x_{i-1})^2+2b_{i-1}(x_i-x_{i-1})&=&c_i-c_{i-1}      \end{eqnarray}\item It must have continuous slopes with the splines on either side,      \begin{eqnarray}        \ddot S_i(x)        &=&6a_i(x-x_i)+2b_i      \end{eqnarray}      and thus      \begin{eqnarray}        \ddot S_{i-1}(x_i) &=& \ddot S_i(x_i) \\        6a_{i-1}(x_i-x_{i-1})+2b_{i-1}&=&6a_i(x_i-x_i)+2b_i \\        6a_{i-1}(x_i-x_{i-1})+2b_{i-1}&=&2b_i \\        6a_{i-1}(x_i-x_{i-1})&=&2b_i-2b_{i-1} \\        a_{i-1}&=&\frac{2b_i-2b_{i-1}}{6(x_i-x_{i-1})}       \end{eqnarray}\end{enumerate}We could just stick the four resulting equations into a big matrix and solve, but that is not very efficient as there are $4n-4$ equations to be solved, it would be nice to simplify things.  We note that the term $2b_i$ appears a lot, so we make the following definition\begin{eqnarray}M_i &=& 2b_i\end{eqnarray}and then solve for each of the four unknowns in terms of the new variable and the $x_k$ and $y_k$ terms.  The four equations we have from above with our definition make five key equations for our problem\begin{enumerate}\item $d_i=y_i$\item $c_i=\frac{y_{i+1}-y_i}{x_{i+1}-x_i}-a_i(x_{i+1}-x_i)^2-b_i(x_{i+1}-x_i)$\item $3a_{i-1}(x_i-x_{i-1})^2+2b_{i-1}(x_i-x_{i-1})=c_i-c_{i-1}$\item $a_i=\frac{2b_{i+1}-2b_i}{6(x_{i+1}-x_i)}$\item $b_i=\frac{1}{2}M_i$\end{enumerate}The first one is already ok, as is the fifth (the definition).  The fourth one is easy to put into the new variables\begin{eqnarray}a_i&=&\frac{2b_{i+1}-2b_i}{6(x_{i+1}-x_i)}\\&=&\frac{M_{i+1}-M_i}{6(x_{i+1}-x_i)}.\end{eqnarray}We now have ways to calculate $a_i$, $b_i$, and $d_i$ in the new variables.  We only need an equation for the $c_i$ and another to find the $M_i$.  Conveniently we still have two more equations.  The second one in our list gives $c_i$ in terms of $a_i$ and $b_i$ but these are known in terms of $M_i$ so we can just substitute\begin{eqnarray}c_i&=&\frac{y_{i+1}-y_i}{x_{i+1}-x_i}-a_i(x_{i+1}-x_i)^2-b_i(x_{i+1}-x_i)\\&=&\frac{y_{i+1}-y_i}{x_{i+1}-x_i}-\left(\frac{M_{i+1}-M_i}{6(x_{i+1}-x_i)}\right)(x_{i+1}-x_i)^2-\left(\frac{1}{2}M_i\right)(x_{i+1}-x_i)\\&=&\frac{y_{i+1}-y_i}{x_{i+1}-x_i}-\left(\frac{M_{i+1}-M_i}{6}+\frac{1}{2}M_i\right)(x_{i+1}-x_i)\\&=&\frac{y_{i+1}-y_i}{x_{i+1}-x_i}-\left(\frac{1}{6}M_{i+1}+\frac{1}{3}M_i\right)(x_{i+1}-x_i)\end{eqnarray}Now we only need to find the values of the $M_i$ and we can then find the values of the other variables, thus instead of solving for $4n-4$ unknowns we only need to find $n-1$ unknowns.  Since solving for them will take $n^3$ operations, all that math cut the work down by $4^3$ or to a sixty fourth of the brute force idea.  The equation for the $M_i$ comes from equation 3 in the list.  First let's simplify the left hand side.\begin{eqnarray}3a_{i-1}(x_i-x_{i-1})^2+2b_{i-1}(x_i-x_{i-1})&=&c_i-c_{i-1}\\3\frac{M_i-M_{i-1}}{6(x_i-x_{i-1})}(x_i-x_{i-1})^2+M_{i-1}(x_i-x_{i-1})&=&c_i-c_{i-1}\\\frac{M_i-M_{i-1}}{2}(x_i-x_{i-1})+M_{i-1}(x_i-x_{i-1})&=&c_i-c_{i-1}\\(M_i+M_{i-1})\frac{x_i-x_{i-1}}{2}&=&c_i-c_{i-1}\end{eqnarray}Now lets simplify the right hand side.\begin{eqnarray}(M_i+M_{i-1})\frac{x_i-x_{i-1}}{2}&=&\left(\frac{y_{i+1}-y_i}{x_{i+1}-x_i}-\left(\frac{1}{6}M_{i+1}+\frac{1}{3}M_i\right)(x_{i+1}-x_i)\right)\nonumber\\&&\quad-\left(\frac{y_i-y_{i-1}}{x_i-x_{i-1}}-\left(\frac{1}{6}M_i+\frac{1}{3}M_{i-1}\right)(x_i-x_{i-1})\right)\\&=&\left(\frac{y_{i+1}-y_i}{x_{i+1}-x_i}-\frac{y_i-y_{i-1}}{x_i-x_{i-1}}\right)\nonumber\\&&\quad-\frac{1}{6}M_{i+1}(x_{i+1}-x_i)-\frac{1}{3}M_i(x_{i+1}-x_i)\nonumber\\&&\quad +\frac{1}{6}M_i(x_i-x_{i-1})+\frac{1}{3}M_{i-1}(x_i-x_{i-1})\end{eqnarray}Now collect terms in $M_i$ onto the left hand side, leaving the terms with no $M_i$ on the right.\begin{eqnarray}\left(\frac{x_{i+1}-x_i}{6}\right)M_{i+1}+&&\nonumber\\\left(\frac{x_i-x_{i-1}}{2}-\frac{x_i-x_{i-1}}{6}+\frac{x_{i+1}-x_i}{3}\right)M_{i}+&&\nonumber\\\left(\frac{x_i-x_{i-1}}{2}-\frac{x_i-x_{i-1}}{3}\right)M_{i-1}&=&\frac{y_{i+1}-y_i}{x_{i+1}-x_i}-\frac{y_i-y_{i-1}}{x_i-x_{i-1}}\\\frac{x_{i+1}-x_i}{6}M_{i+1}+\frac{x_{i+1}-x_{i-1}}{3}M_{i}+\frac{x_i-x_{i-1}}{6}M_{i-1}&=&\frac{y_{i+1}-y_i}{x_{i+1}-x_i}-\frac{y_i-y_{i-1}}{x_i-x_{i-1}}\end{eqnarray}The only thing that we need is to calculate $M_{i}$ for the naturalcubic spline, which is done byrequiring $M_{1}=M_{n}=0$ and solving the following matrix system\beqnAx & = & b \\A & = &\left[\begin{matrix}\alpha_{2} & \beta_{2}  &  0        & \cdots       & 0 \cr\beta_{2}  & \alpha_{3} & \beta_{3} & \ddots       & \vdots \cr0          & \beta_{3}  & \ddots    & \ddots       & 0 \cr\vdots     & \ddots     & \ddots    & \alpha_{n-2} & \beta_{n-2} \cr0          & \cdots     & 0         & \beta_{n-2}  & \alpha_{n-1}\end{matrix}\right] \\x & = &\left[\begin{matrix}M_{2} \cr\vdots \crM_{n-1}\end{matrix}\right] \qquadb =\left[\begin{matrix}\gamma_{2}-\gamma_{1} \cr\vdots \cr\gamma_{n-1}-\gamma_{n-2}\end{matrix}\right] \\\alpha_{i} & = & \frac{x_{i+1}-x_{i-1}}{3}\qquad\beta_{i} = \frac{x_{i+1}-x_{i}}{6}\qquad\gamma_{i} = \frac{y_{i+1}-y_{i}}{x_{i+1}-x_{i}}\eeqnWhen we have found the $M_i$ we can then substitute these values to find the original splines, and thus plot them.\begin{eqnarray}S_i(x)&=&a_i(x-x_i)^3+b_i(x-x_i)^2+c_i(x-x_i)+d_i\\a_i &=& \frac{M_{i+1}-M_i}{6(x_{i+1}-x_i)}\\b_i &=& \frac{1}{2}M_i\\c_i &=& \frac{y_{i+1}-y_i}{x_{i+1}-x_i}-\left(\frac{1}{6}M_{i+1}+\frac{1}{3}M_i\right)(x_{i+1}-x_i)\\d_i &=& y_i\end{eqnarray}\subsection{Not-a-Knot Cubic Spline}We can also find the $M_{i}$ for the not-a-knot cubic spline, which isoften preferred by solving a similar system\beqnAx & = & b \\A & = &\left[\begin{matrix}\psi_{1}   & \beta_{1}  &  0         &  0         & \cdots       & 0            & 0\cr\beta_{1}  & \alpha_{2} & \beta_{2}  &  0         & \cdots       & 0            & 0 \cr0          & \beta_{2}  & \alpha_{3} & \beta_{3}  & \ddots       & \vdots       & \vdots \cr0          & 0          & \beta_{3}  & \ddots     & \ddots       & 0            & 0 \cr\vdots     & \vdots     & \ddots     & \ddots     & \alpha_{n-2} & \beta_{n-2}  & 0 \cr0          & 0          & \cdots     & 0          & \beta_{n-2}  & \alpha_{n-1} & \beta_{n-1} \cr0          & 0          & \cdots     & 0          & 0            & \beta_{n-1}  & \phi_{2}\end{matrix}\right] \\x & = &\left[\begin{matrix}M_{1} \crM_{2} \cr\vdots \crM_{n-1} \crM_{n}\end{matrix}\right] \qquadb =\left[\begin{matrix}\gamma_{1}-f'(x_{1}) \cr\gamma_{2}-\gamma_{1} \cr\vdots \cr\gamma_{n-1}-\gamma_{n-2} \crf'(x_{n})-\gamma_{n-1}\end{matrix}\right] \\\alpha_{i} & = & \frac{x_{i+1}-x_{i-1}}{3}\qquad\beta_{i} = \frac{x_{i+1}-x_{i}}{6}\qquad\gamma_{i} = \frac{y_{i+1}-y_{i}}{x_{i+1}-x_{i}} \\\psi_{1} & = & \frac{x_{2}-x_{1}}{3}\qquad\phi_{2} = \frac{x_{n}-x_{n-1}}{3}\eeqnor (if you don't know the derivative)\beqnAx & = & b \\A & = &\left[\begin{matrix}\psi_{1}   & \psi_{2}   &  0         &  0         & \cdots       & 0            & 0 \cr\beta_{1}  & \alpha_{2} & \beta_{2}  &  0         & \cdots       & 0            & 0 \cr0          & \beta_{2}  & \alpha_{3} & \beta_{3}  & \ddots       & \vdots       & \vdots \cr0          & 0          & \beta_{3}  & \ddots     & \ddots       & 0            & 0 \cr\vdots     & \vdots     & \ddots     & \ddots     & \alpha_{n-2} & \beta_{n-2}  & 0 \cr0          & 0          & \cdots     & 0          & \beta_{n-2}  & \alpha_{n-1} & \beta_{n-1} \cr0          & 0          & \cdots     & 0          & 0            & \phi_{2}     & \phi_{1}\end{matrix}\right] \\x & = &\left[\begin{matrix}M_{1} \crM_{2} \cr\vdots \crM_{n-1} \crM_{n}\end{matrix}\right] \qquadb =\left[\begin{matrix}\psi_{3} \cr\gamma_{2}-\gamma_{1} \cr\vdots \cr\gamma_{n-1}-\gamma_{n-2} \cr\phi_{3}\end{matrix}\right] \\\alpha_{i} & = & \frac{x_{i+1}-x_{i-1}}{3}\qquad\beta_{i} = \frac{x_{i+1}-x_{i}}{6}\qquad\gamma_{i} = \frac{y_{i+1}-y_{i}}{x_{i+1}-x_{i}} \\\xi_{1} & = & x_{2}-x_{1}\qquad\xi_{2} = x_{2}-z_{1}\qquad\xi_{3} = z_{1}-x_{1} \\\psi_{1} & = & \frac{\xi_{2}^{3}-\xi_{1}^{2}\xi_{2}}{6\xi_{1}}\qquad\psi_{2} = \frac{\xi_{3}^{3}-\xi_{1}^{2}\xi_{3}}{6\xi_{1}}\qquad\psi_{3} = f(z_{1})-\frac{\xi_{2}y_{1}+\xi_{3}y_{2}}{\xi_{1}} \\\xi_{4} & = & x_{n}-x_{n-1}\qquad\xi_{5} = x_{n}-z_{2}\qquad\xi_{6} = z_{2}-x_{n-1} \\\phi_{1} & = & \frac{\xi_{5}^{3}-\xi_{4}^{2}\xi_{5}}{6\xi_{4}}\qquad\phi_{2} = \frac{\xi_{6}^{3}-\xi_{4}^{2}\xi_{6}}{6\xi_{4}}\qquad\phi_{3} = f(z_{2})-\frac{\xi_{5}y_{n-1}+\xi_{6}y_{n}}{\xi_{4}}\eeqnNote, you can easily enter the matrix $A$ into Matlab by using thecommand diag.  For instance, if you put the entries of $A$ that are onthe main diagonal into the vector $A1$, the first sub-diagonal into$A2$, and the first super-diagonal into $A3$, then in Matlab you enter,{\it A=diag(A1)+diag(A2,-1)+diag(A3,1);}.Homeworksection 5.3: 7section 5.4: 3, 5