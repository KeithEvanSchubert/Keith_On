\chapter{Integration}\label{c-Integ}The fundamental theorem of Calculus tells us that an integral of afunction can be expressed in terms of the anti-derivative of thefunction.  Unfortunately, not all functions have anti-derivativesthat are expressible in known functions.  One of the most famous isthe Gaussian probability distribution, which is given by\beqne^{-\left(\frac{x-\mu}{\sigma}\right)^{2}}.\eeqnThe anti-derivative of this important and frequently occurring functionis unknown.  How do we handle it?  That is the subject of this chapter.\section{Riemann}We recall from Calculus that the integral is defined as\beqn\int_{a}^{b}{f(x)dx} =\lim_{n\rightarrow\infty}\sum_{j=1}^{n}f(p_{j})(x_{j}-x_{j-1}).\eeqnNow assume that all $n$ of the $x_{j}$ are evenly spaced on $[a,b]$.  Wecan then write\beqnh & = & \frac{b-a}{n} \\  & = & x_{j}-x_{j-1}.\eeqnWe can use this to get an expression for the Riemann Sum\beqn\int_{a}^{b}{f(x)dx} & = &\lim_{n\rightarrow\infty}\sum_{j=1}^{n}f(p_{j})(x_{j}-x_{j-1}) \\ & = &\lim_{n\rightarrow\infty}\sum_{j=1}^{n}f(p_{j})h \\ & = &\lim_{n\rightarrow\infty}h\sum_{j=1}^{n}f(p_{j}).\eeqnTo evaluate the integral numerically we are not able to take thelimit, so we get\beqn\int_{a}^{b}{f(x)dx} & \approx &h\sum_{j=1}^{n}f(p_{j}).\eeqnThe exact size of $n$ for the approximation to be good is a key aspectof numerical integration.  Note also that I have not specified what$p_{j}$ is, as this form allows you to do a left, right, mid-point,maximum, or minimum.  The basic idea here is that we are approximatingthe function by a constant on the interval.\beqn\int_{a}^{b}{f(x)dx} & \approx &h\sum_{j=1}^{n}f(p_{j}).\eeqnNow we want to think about the error.  First we want to get anexpression for the integral in terms of an exact sum.  To do this weuse the fundamental theorem of calculus, add nothing, rearrange, anduse the mean value theorem.\beqn\int_{a}^{b}f(x)dx& = &F(b)-F(a) \\& = &F(x_{n})-F(x_{0}) \\& = &F(x_{n})+\sum_{i=1}^{n-1}(F(x_{i})-F(x_{i}))-F(x_{0}) \\& = &\sum_{i=1}^{n}(F(x_{i})-F(x_{i-1})) \\& = &\sum_{i=1}^{n}f(c_{i})(x_{i}-x_{i-1}) \\& = &\sum_{i=1}^{n}f(c_{i})h \\& = &h\sum_{i=1}^{n}f(c_{i})\eeqnThis expression is exact so we can take it and subtract theexpression for the midpoint method.  We will assume the function has aderivative and we will note that since we have picked the midpointthat any other point in the interval is within half the width of theinterval of the midpoint.\beqnE& = &h\sum_{i=1}^{n}f(c_{i}) - h\sum_{i=1}^{n}f(p_{i}) \\& = &h\sum_{i=1}^{n}(f(c_{i}) - f(p_{i})) \\& = &h\sum_{i=1}^{n}f'(d_{i})(c_{i} - p_{i}) \\& \leq &h\sum_{i=1}^{n}f'(d_{i})\frac{h}{2} \\& = &\frac{h^{2}}{2}\sum_{i=1}^{n}f'(d_{i})\eeqnRecall that $h$ is inversely proportional to $n$, so we have that theError is inversely proportional to the square of $n$.\section{Trapezoid}\beqn\int_{a}^{b}{f(x)dx} & \approx &h\left(\frac{f(x_{0})+f(x_{n})}{2}+\sum_{j=1}^{n}f(x_{j})\right)\eeqn\section{Simpson}\beqn\int_{a}^{b}{f(x)dx} & \approx &\frac{h}{3}\left(f(x_{0})+f(x_{n})+2\sum_{j=1}^{\frac{n}{2}-1}f(x_{2j})+4\sum_{j=1}^{\frac{n}{2}}f(x_{2j-1})\right)\eeqn\section{Richardson}Define the value of the integral to be $I$ and the numericapproximation by some method at $n$ node points to be $I_{n}$.  We note that the error is of the form\beqnE& = &I-I_{n} \\& = &\frac{c}{n^{p}},\eeqnwith $c$ a constant dependent on the function and $p$ a power dependenton the method.  For midpoint and trapezoidal methods $p=2$, while forSimpson's method $p=4$.  If we double the number of points then theerror would be\beqnE& = &I-I_{2n} \\& = &\frac{c}{2^{p}n^{p}} \\& = &\frac{1}{2^{p}}(I-I_{n}).\eeqnSolving for $I$ we find\beqnI-I_{2n}& = &\frac{1}{2^{p}}(I-I_{n}) \\I-\frac{1}{2^{p}}I& = &I_{2n}\frac{1}{2^{p}}I_{n} \\\frac{2^{p}-1}{2^{p}}I& = &I_{2n}-\frac{1}{2^{p}}I_{n} \\I& = &\frac{2^{p}}{2^{p}-1}I_{2n}-\frac{1}{2^{p}-1}I_{n} \\I& = &\frac{1}{2^{p}-1}\left(2^{p}I_{2n}-I_{n}\right).\eeqnThis is Richardson's Extrapolation formula, and it will typicallygive a big improvement to any of the methods.  We can use thisestimate of the error to calculate the error\beqnE& = &I-I_{2n} \\& = &\frac{2^{p}}{2^{p}-1}I_{2n}-\frac{1}{2^{p}-1}I_{n}-I_{n} \\& = &\frac{2^{p}-(2^{p}-1)}{2^{p}-1}I_{2n}-\frac{1}{2^{p}-1}I_{n} \\& = &\frac{1}{2^{p}-1}I_{2n}-\frac{1}{2^{p}-1}I_{n} \\& = &\frac{1}{2^{p}-1}(I_{2n}-I_{n}).\eeqnThis is Richardson's error formula.  We note that we can get aconvergence rate by doing a little algebra on what we have alreadyfound.\beqnI-I_{2n}& = &\frac{1}{2^{p}}(I-I_{n}) \\\frac{I-I_{n}}{I-I_{2n}}& = &2^{p}\eeqnThis is a nice equation but in general it is not calculable, as wedon't know $I$ and might not know p.  We can handle this by consideringthe quantity\beqn\frac{I_{2n}-I_{n}}{I_{4n}-I_{2n}}& = &\frac{I_{2n}-I_{n}+I-I}{I_{4n}-I_{2n}+I-I} \\& = &\frac{(I-I_{n})-(I-I_{2n})}{(I-I_{2n})-(I-I_{4n})} \\& = &\frac{(I-I_{n})-\frac{1}{2^{p}}(I-I_{n})}{\frac{1}{2^{p}}(I-I_{n})-\frac{1}{4^{p}}(I-I_{n})} \\& = &\frac{1-\frac{1}{2^{p}}}{\frac{1}{2^{p}}(1-\frac{1}{2^{p}})} \\& = &\frac{1}{\frac{1}{2^{p}}} \\& = &2^{p}.\eeqnWe thus have a simple way of calculating the convergence rate, andthus a way to find $p$.Homework: 7.1) 2(a), (b), (f) for midpoint, trapezoidal, andsimpson.  Compare with the error for trapezoidal (7.32) andRichardson's extrapolation.7.2) 8\section{Gaussian Quadrature}And now for something completely different\ldotsLast time we considered the standard way of thinking about integration, namely summing up a bunch of small areas.  The technique we will discuss today was introduced in 1814 by Gauss, hence the name.  We will now consider the integral\beqnI(f) = \int_{-1}^{1}f(t)dt.\eeqnNote that this is a perfectly general statement, as all we need to do to convert this to an integral of the form $\int_a^bf(x)dx$ is to find a linear mapping between the two intervals of integration:\begin{eqnarray}t&=& [-1,1]\\x&=& [a,b]\end{eqnarray}We thus want to find a function $x=g(t)$ such that $a=g(-1)$, $b=g(1)$, and $g(\cdot)$ is linear.  We have two points which determine a line, so we can use the two-point formula for a line:\begin{eqnarray}m_g&=& \frac{x_1-x_0}{t_1-t_0}\\&=& \frac{b-a}{1--1}\\&=& \frac{b-a}{2}\\x&=& x_1+m_g(t-t_1)\\&=& b+\frac{b-a}{2}(t-1)\\&=& \frac{1}{2}(2b+t(b-a)-(b-a))\\&=& \frac{1}{2}(t(b-a)+b+a)\\&=& \frac{t(b-a)+b+a}{2}\end{eqnarray}So, if we define $x=0.5(t(b-a)+b+a)$ and thus $dx=0.5(b-a)dx$ then we have\footnote{You will see some books define $t=(2x-a-b)/(b-a)$, and $dt=2dx/(b-a)$, which is the same formula but not as convenient for what we want.}\beqn\int_{a}^{b}{f(x)dx} & = &\int_{g(-1)}^{g(1)}{f(x)g'(t)dt}\\ & = &\int_{-1}^{1}{f\left(\frac{(b-a)t+b+a}{2}\right)\left(\frac{b-a}{2}\right)dt}\eeqnWe have that the integral is general, if not all that obvious as to why we chose this in the first place (it will become more apparent in a few moments).  We still need to show how to estimate the integral.  The basic idea is to approximate the integral by weighted evaluations of the function at a series of node points.  To get a good estimate we will require that our estimate at $n$ node points will be good for every polynomial up to order $2n-1$.  How?  Pick $w_{i}$ and $x_{i}$ such that\beqnI(f)=\sum_{i=1}^{n}w_{i}f(x_{i})\eeqnholds for all $f\in\{1,x,\ldots,x^{2n-1}\}$.  Let's do some examples.\vspace{.1in}\noindent\textbf{Example:}Consider $n=1$.\beqn\int_{-1}^{1}dx & = &w_{1}f(x_{1}) \\2 & = &w_{1} \\\int_{-1}^{1}xdx & = &w_{1}x_{1} \\0 & = &2x_{1} \\0 & = &x_{1}\eeqnAlso, consider $n=2$.\beqn\int_{-1}^{1}dx & = &w_{1}f(x_{1})+w_{2}f(x_{2}) \\2 & = &w_{1}+w_{2} \\\int_{-1}^{1}xdx & = &w_{1}f(x_{1})+w_{2}f(x_{2}) \\0 & = &w_{1}x_{1}+w_{2}x_{2} \\\int_{-1}^{1}x^{2}dx & = &w_{1}f(x_{1})+w_{2}f(x_{2}) \\\frac{2}{3} & = &w_{1}x_{1}^{2}+w_{2}x_{2}^{2} \\\int_{-1}^{1}x^{3}dx & = &w_{1}f(x_{1})+w_{2}f(x_{2}) \\0 & = &w_{1}x_{1}^{3}+w_{2}x_{2}^{3}\eeqnSolve these four equations for four unknowns and we find\beqnw_{1} & = & 1 \\w_{2} & = & 1 \\x_{1} & = & -\frac{1}{\sqrt{3}} \\x_{1} & = & \frac{1}{\sqrt{3}}%w_{2} & = & 2-w_{1} \\%x_{2} & = & -\frac{w_{1}}{2-w_{1}}x_{1} \\%w_{1} & = & -(2-w_{1})\left(\frac{-\frac{w_{1}}{2-w_{1}}x_{1}}{x_{1}}\right)^{2} \\%x_{1} & = & \left(%  \frac{2-w_{1}}{-(2-w_{1})\left(\frac{-\frac{w_{1}}{2-w_{1}}x_{1}}{x_{1}}\right)^{2}}%  \right)^{\frac{1}{3}}\eeqnThe node points turn out to be the roots of the Legendrepolynomials.  The Legendre polynomials are defined on $[-1,1]$ henceour choice for the limits of integration.  You can find the Legendrepolynomials by the following properties.\begin{itemize}\item$P_{n}$ is a polynomial of order $n$.\itemThey are orthogonal\beqn\int_{-1}^{1}P_{i}(x)P_{j}(x)dx=0\eeqnwhen $i\ne j$.\itemThe normalization is\beqn\int_{-1}^{1}P_{n}(x)^{2}dx=\frac{2}{2n+1}\eeqn\end{itemize}The first several Legendre polynomials are$\{1,x,x^{2}-\frac{1}{3},x^{3}-\frac{3}{5}x,x^{4}-\frac{6}{7}x^{2}+\frac{3}{35}\}$.The constants can be found by\beqnw_{i}=\int_{-1}^{1}\prod_{j=1, j\ne i}^{n}\frac{x-x_{j}}{x_{i}-x_{j}}dx\eeqnIn general we do not need to use this as the values are well tabulated, see for instance Table~\ref{t-gauss-quad}.\begin{table}  \centering  \caption{Gauss-Legendre Abscissae and Weights to 8 Decimal Places}\label{t-gauss-quad}\begin{tabular}{rr@{}llc}n  & \multicolumn{2}{c}{Evaluation Points, $x_i$} & Weights, $w_i$ & Degree (2n-1) \\ \hline1  & 0&.0                                         & 2.0            & 1 \\ \hline2  & $\pm$&$\frac{1}{\sqrt{3}}$                   & 1.0            & 3 \\ \hline3  & 0&.0                                         & 0.88888889     & 5 \\   & $\pm$0&.77459667                             & 0.55555555     &   \\ \hline4  & $\pm$0&.33998104                             & 0.65214515     & 7 \\   & $\pm$0&.86113631                             & 0.34785485     &   \\ \hline5  & 0&.0                                         & 0.56888889     & 9 \\   & $\pm$0&.53846931                             & 0.47862867     &   \\   & $\pm$0&.90617985                             & 0.23692689     &   \\ \hline6  & $\pm$0&.23861918                             & 0.46791393     & 11 \\   & $\pm$0&.66120939                             & 0.36076157     &   \\   & $\pm$0&.93246951                             & 0.17132449     &   \\ \hline7  & 0&.0                                         & 0.41795918     & 13 \\   & $\pm$0&.40584515                             & 0.38183005     & \\   & $\pm$0&.74153119                             & 0.27970539     & \\   & $\pm$0&.94910791                             & 0.12948497     & \\ \hline8  & $\pm$0&.18343464                             & 0.36268378     & 15 \\   & $\pm$0&.52553241                             & 0.31370665     & \\   & $\pm$0&.79666648                             & 0.22238103     & \\   & $\pm$0&.96028986                             & 0.10122854     & \\ \hline10 & $\pm$0&.14887434                             & 0.29552422     & 19 \\   & $\pm$0&.43339539                             & 0.26926672     & \\   & $\pm$0&.67940957                             & 0.21908636     & \\   & $\pm$0&.86506337                             & 0.14945135     & \\   & $\pm$0&.97390653                             & 0.06667134     & \\ \hline\end{tabular}\end{table}Homework Redo 7.1) 2(a), (b), (f) for gaussian quadrature up to$n=8$.  How does the convergence compare?